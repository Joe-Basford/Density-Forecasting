{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8eb76bf",
   "metadata": {},
   "source": [
    "# Creating the MCS (Hansen, Lunde, Nason ; 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#######################################################################\n",
    "######################### Importing Packages ##########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.stats\n",
    "\n",
    "# need to clean these up later\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Activation, Input, Dense, LSTM, concatenate\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from numpy.random import rand\n",
    "from numpy import ix_\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df900fd1",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07072ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Neural network and GARCH models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bc947",
   "metadata": {},
   "source": [
    "# The MCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gw(L: np.array,\n",
    "       tau: int,\n",
    "       H: Optional[np.array] = None,\n",
    "       kernel: Optional[Union[str, Callable]] = None,\n",
    "       bw: Optional[int] = None,\n",
    "       kernel_kwargs: Optional[dict] = None,\n",
    "       alpha: float = 0.05) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Test of Equal Conditional Predictive Ability by Giacomini and White (2006).\n",
    "    Used here for testing and debugging but made available through the package interface.\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Giacomini (https://gist.github.com/ogrnz/91f37140011d1c2447934766274c4070)\n",
    "    References:\n",
    "        - Giacomini, R., & White, H. (2006). Tests of conditional predictive ability. Econometrica, 74(6), 1545-1578.\n",
    "    :param L: (Tx2) array of forecast losses\n",
    "    :param H: (Txq) array of instruments. If `None` provided, defaults to the unconditional EPA (DM test)\n",
    "    :param tau: Forecast horizon\n",
    "    :param kernel: (default `None`)\n",
    "        If multistep forecast (`tau` > 1), covariance matrix is an HAC estimator.\n",
    "        Original implementation uses a Bartlett kernel with bandwidth `tau - 1`.\n",
    "        If a `str`, must match one of `arch` package variance estimator:\n",
    "         > https://arch.readthedocs.io/en/latest/covariance/covariance.html\n",
    "        If a `Callable`, must simply return a (qxq) covariance matrix (see arg `H`).\n",
    "    :param bw: (default `None`)\n",
    "        Bandwidth of the `kernel`. Typically set to `forecasting horizon - 1`.\n",
    "        If set to `None`, will let the kernel compute the optimal bandwidth if supported.\n",
    "    :param kernel_kwargs: (default `None`)\n",
    "        An optional dict of `argument: value` passed to `kernel`.\n",
    "        If `kernel` is a `Callable`, the eventual bandwidth must be passed here.\n",
    "    :param alpha: Significance level\n",
    "    :return: tuple(S, crit_val, p_val)\n",
    "        S: test statistic,\n",
    "        cval: critical value for significance lvl,\n",
    "        pval: p-value of test\n",
    "    \"\"\"\n",
    "    T = L.shape[0]  # Number of observations\n",
    "    d = L[:, 0] - L[:, 1]  # Loss differential\n",
    "    if H is None:  # Instruments (defaults to unconditional EPA)\n",
    "        H = np.ones((T, 1))\n",
    "    q = H.shape[1]\n",
    "\n",
    "    reg = np.empty(H.shape)\n",
    "    for jj in range(H.shape[1]):\n",
    "        reg[:, jj] = H[:, jj] * d\n",
    "\n",
    "    if tau == 1:  # One-step\n",
    "        beta = np.linalg.lstsq(reg, np.ones(T), rcond=None)[0][0].item()\n",
    "        res = np.ones(T) - beta * reg\n",
    "        r2 = 1 - np.mean(res ** 2)\n",
    "        S = T * r2\n",
    "    else:  # Multistep\n",
    "        omega = np.empty((q, q))  # Defined here to make linter happy and inform about dims\n",
    "        if isinstance(kernel, Callable):  # Custom callable\n",
    "            omega = kernel(reg, **kernel_kwargs)\n",
    "        elif isinstance(kernel, str):  # Arch covariance\n",
    "            kerfunc = getattr(kernels, kernel)\n",
    "            ker = kerfunc(reg, bandwidth=bw, **kernel_kwargs)\n",
    "            omega = ker.cov.long_run\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        zbar = reg.mean().T\n",
    "        S = (T * zbar.T * np.linalg.pinv(omega) * zbar).item()\n",
    "\n",
    "    dof = reg.shape[1]\n",
    "    cval = chi2.ppf(1 - alpha, dof)\n",
    "    pval = 1 - chi2.cdf(abs(S), dof)\n",
    "\n",
    "    return S, cval, pval\n",
    "\n",
    "\n",
    "def mgw(L: np.array,\n",
    "        H: Optional = None,\n",
    "        covar_style: Literal[\"sample\", \"hac\"] = \"sample\",\n",
    "        kernel: Optional[Union[str, Callable]] = None,\n",
    "        bw: Optional[int] = None,\n",
    "        kernel_kwargs: Optional[dict] = None,\n",
    "        alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Implements the multivariate Giacomini-White (MGW) (Borup et al., 2022) test of equal predictive ability.\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Borup (https://sites.google.com/view/danielborup/research)\n",
    "    Notes:\n",
    "        If only 2 models are compared, it reduces to the Giacomini-White test (GW) (Giacomini and White, 2006)\n",
    "        If further no conditioning information H is given, it reduces to the\n",
    "        original Diebold-Mariano test (DM) (Diebold and Mariano, 1995)\n",
    "        If more than 2 models are compared but with no conditioning information H,\n",
    "        it reduces to multivariate Diebold-Mariano (MDM) (Mariano and Preve, 2012)\n",
    "    References:\n",
    "        - Borup, Daniel and Eriksen, Jonas Nygaard and Kjær, Mads Markvart and Thyrsgaard, Martin,\n",
    "        Predicting Bond Return Predictability. Available at http://dx.doi.org/10.2139/ssrn.3513340\n",
    "        - Diebold, F.X., and R.S. Mariano (1995) ‘Comparing Predictive Accuracy,’ Journal\n",
    "        of Business and Economic Statistics 13, 253–263.\n",
    "        - Giacomini, R., & White, H. (2006). Tests of conditional predictive ability.\n",
    "        Econometrica, 74(6), 1545-1578.\n",
    "        - Mariano, R. S., & Preve, D. (2012). Statistical tests for multiple forecast comparison.\n",
    "        Journal of econometrics, 169(1), 123-130.\n",
    "    :param L:\n",
    "        Txk matrix of losses of k models with T forecasts.\n",
    "    :param H: (default `None`)\n",
    "        Txq matrix of a constant and information set (test function).\n",
    "        If not provided, set to a (Tx1) column vector of 1, amounts to the\n",
    "        unconditional MWG test, which is equivalent to the multivariate Diebold-Mariano (Mariano and Preve, 2012).\n",
    "    :param covar_style: (default 'sample')\n",
    "        How to compute the covariance matrix.\n",
    "        Either the sample covariance ('sample') or an HAC estimator ('hac').\n",
    "    :param kernel: (default `None`)\n",
    "        If covariance matrix is an HAC estimator, what type to compute.\n",
    "        If a `str`, must match one of `arch` package variance estimator:\n",
    "         > https://arch.readthedocs.io/en/latest/covariance/covariance.html\n",
    "        If a `Callable`, must simply return a\n",
    "    :param bw: (default `None`)\n",
    "        Bandwidth of the `kernel`. Typically set to `forecasting horizon - 1`.\n",
    "        If set to `None`, will let the kernel compute the optimal bandwidth if supported.\n",
    "    :param kernel_kwargs: (default `None`)\n",
    "        An optional dict of `argument: value` passed to `kernel`.\n",
    "        If `kernel` is a `Callable`, the eventual bandwidth must be passed here.\n",
    "    :param alpha: (default 0.05)\n",
    "        Significance level.\n",
    "    :return: tuple(S, cval, pval)\n",
    "        S: float, the computed test statistic\n",
    "        cval: float, the corresponding critical value\n",
    "        pval: float, the p-value of S.\n",
    "    \"\"\"\n",
    "    # Args checks\n",
    "    if kernel is not None and covar_style == \"sample\":\n",
    "        raise ValueError(f\"{kernel=} incompatible with {covar_style=}.\")\n",
    "    if kernel is None and covar_style == \"hac\":\n",
    "        raise ValueError(\"Set `kernel` when using an HAC estimator.\")\n",
    "    if bw is not None and covar_style == \"sample\":\n",
    "        raise ValueError(f\"{bw=} incompatible with {covar_style=}.\")\n",
    "    if L.shape[1] < 2:\n",
    "        raise ValueError(f\"Not enough columns for matrix of  losses {L.shape[1]=}.\")\n",
    "\n",
    "    if kernel_kwargs is None:\n",
    "        kernel_kwargs = {}\n",
    "\n",
    "    # Initialize\n",
    "    T = L.shape[0]\n",
    "    p = L.shape[1] - 1\n",
    "    if H is None:  # defaults to unconditional EPA\n",
    "        H = np.ones((T, 1))\n",
    "\n",
    "    # Loss differentials\n",
    "    D = np.diff(L, axis=1)\n",
    "\n",
    "    # Conditioning information\n",
    "    q = H.shape[1]\n",
    "    reg = np.empty((T, q * p))\n",
    "\n",
    "    for i in range(T):\n",
    "        reg[i, :] = np.kron(H[i, :], D[i, :])\n",
    "\n",
    "    Dbar = np.mean(reg, axis=0)\n",
    "\n",
    "    # Compute covar matrix\n",
    "    omega = np.empty((q * p, q * p))  # Defined here to make linter happy and inform about dims\n",
    "    if covar_style == \"sample\":\n",
    "        omega = (reg - Dbar).T @ (reg - Dbar) / (T - 1)\n",
    "    elif covar_style == \"hac\":  # HAC estimator\n",
    "        if isinstance(kernel, Callable):  # Custom callable\n",
    "            omega = kernel(reg, **kernel_kwargs)\n",
    "        elif isinstance(kernel, str):  # Arch covariance\n",
    "            kerfunc = getattr(kernels, kernel)\n",
    "            ker = kerfunc(reg, bandwidth=bw, **kernel_kwargs)\n",
    "            omega = ker.cov.long_run\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Compute statistic\n",
    "    dof = q * p\n",
    "    S = (T * Dbar @ np.linalg.pinv(omega) @ Dbar.T).item()\n",
    "    cval = chi2.ppf(1 - alpha, dof)\n",
    "    pval = 1 - chi2.cdf(S, dof)\n",
    "\n",
    "    return S, cval, pval\n",
    "\n",
    "\n",
    "def cmcs(L: np.array, H: Optional = None, alpha: float = 0.05, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform the Conditional Model Confidence Set (CMCS).\n",
    "    The MCS procedure from Hansen (2011) is adapted to use MGW (Borup et al., 2022)\n",
    "    instead of bootstrapping the critical values. Allows to reduce an initial set of models to a\n",
    "    set of models with equal (conditional) predictive ability.\n",
    "    Also, allows to use conditioning information (`H`, hence the 'Conditional'),\n",
    "    to get the best MCS based on expected future loss.\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Borup (https://sites.google.com/view/danielborup/research)\n",
    "    References:\n",
    "        - Borup, Daniel and Eriksen, Jonas Nygaard and Kjær, Mads Markvart and Thyrsgaard, Martin,\n",
    "        Predicting Bond Return Predictability. Available at http://dx.doi.org/10.2139/ssrn.3513340\n",
    "        - Hansen, P. R., Lunde, A., & Nason, J. M. (2011). The model confidence set. Econometrica, 79(2), 453-497.\n",
    "    :param L:\n",
    "        (Txk) matrix of losses of k models with T forecasts.\n",
    "    :param H: (default `None`)\n",
    "        (Txq) matrix of a constant and information set (test function).\n",
    "        If not provided, set to a (Tx1) column vector of 1, amounts to the\n",
    "        unconditional MWG test, which is equivalent to the multivariate Diebold-Mariano (Mariano and Preve, 2012).\n",
    "    :param alpha: (default 0.05)\n",
    "        Significance level used in the MGW test.\n",
    "    :param **kwargs: Arguments passed to `feval.mgw`. Usually define covariance estimator and such.\n",
    "    :return: tuple(mcs, S, cval, pval, removed)\n",
    "        mcs: (1xk) np.array where models included in the best model confidence set are noted as 1.\n",
    "        S: float, the computed test statistic of the last test.\n",
    "        cval: float, the corresponding critical value.\n",
    "        pval: float, the p-value of S.\n",
    "        removed: (1xk) np.array where a column represents an algorithm cycle.\n",
    "            That way, we can see which model index was removed at which iteration.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    T = L.shape[0]\n",
    "    k = L.shape[1]\n",
    "    if H is None:\n",
    "        H = np.ones((T, 1))\n",
    "\n",
    "    # Init loop\n",
    "    S, cval, pval = np.inf, 1, 1\n",
    "    mcs = np.ones((1, k))\n",
    "    removed = np.zeros((1, k))\n",
    "\n",
    "    j = 0\n",
    "    while S > cval:\n",
    "        # Create L_to_use, the losses of models still in MCS\n",
    "        L_to_use = L[:, (mcs == 1)[0]]\n",
    "\n",
    "        if L_to_use.shape[1] == 1:  # Only 1 model left in set\n",
    "            break\n",
    "\n",
    "        # Perform MGW\n",
    "        S, cval, pval = mgw(L_to_use, H, alpha=alpha, **kwargs)\n",
    "\n",
    "        # H0 still rejected, apply elimination criterion\n",
    "        if S > cval:\n",
    "            mcs, removed[0, j] = elim_rule(L, mcs, H)\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    return mcs, S, cval, pval, removed\n",
    "\n",
    "\n",
    "def elim_rule(L: np.array,\n",
    "              mcs: np.array,\n",
    "              H: Optional = None):\n",
    "    \"\"\"\n",
    "    Elimination rule that allows to rank losses based on expected future loss given the information set `H`.\n",
    "    If `H` is a vector of constant, it amounts to ranking losses based on average loss.\n",
    "    See Borup et al. (2022) and Hansen (2011).\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Borup (https://sites.google.com/view/danielborup/research)\n",
    "    References:\n",
    "        - Borup, Daniel and Eriksen, Jonas Nygaard and Kjær, Mads Markvart and Thyrsgaard, Martin,\n",
    "        Predicting Bond Return Predictability. Available at http://dx.doi.org/10.2139/ssrn.3513340\n",
    "        - Hansen, P. R., Lunde, A., & Nason, J. M. (2011). The model confidence set. Econometrica, 79(2), 453-497.\n",
    "    :param L:\n",
    "        (Txk) matrix of losses of k models with T forecasts.\n",
    "    :param mcs:\n",
    "        (1xk) vector of current model confidence set, where the least performing model will be eliminated.\n",
    "    :param H: (default `None`)\n",
    "        (Txq) matrix of a constant and information set (test function).\n",
    "    :return: tuple(mcs, removed)\n",
    "        mcs: (1xk) np.array where models included in the best model confidence set are noted as 1.\n",
    "        removed: (1xk) np.array where a column represents an algorithm cycle.\n",
    "            That way, we can see which model index was removed at which iteration.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    k = mcs.shape[1]\n",
    "    q = H.shape[1]\n",
    "    new_k = np.count_nonzero(mcs)\n",
    "\n",
    "    if L.shape[1] != k:\n",
    "        raise ValueError(f\"Dimensions of {L.shape[1]=} do not match {mcs.shape[1]=}.\")\n",
    "\n",
    "    L_to_use = np.zeros((L.shape[0], new_k))\n",
    "    curr_set = np.zeros((1, new_k))\n",
    "    j = 0\n",
    "    for i in range(k):  # TODO could be simplified?\n",
    "        if mcs[0, i] == 1:\n",
    "            L_to_use[:, j] = L[:, i]\n",
    "            curr_set[0, j] = i\n",
    "            j += 1\n",
    "\n",
    "    combinations = np.arange(0, j).reshape(1, -1)  # TODO why matrix? could be vect\n",
    "    L_hat = np.zeros(combinations.shape)\n",
    "\n",
    "    # Estimate\n",
    "    for j in range(combinations.shape[0]):  # TODO no loop if vect\n",
    "        L_intra_use = L_to_use[:, combinations[j, :]]  # TODO directly call j?\n",
    "\n",
    "        deltas = np.zeros((q, new_k - 1))\n",
    "        for i in range(L_to_use.shape[1] - 1):\n",
    "            Y_used = L_intra_use[:, i + 1] - L_intra_use[:, i]\n",
    "            Y_used = Y_used.reshape(-1, 1)\n",
    "            deltas[:, i] = (np.linalg.inv(H.T @ H) @ H.T @ Y_used).reshape(-1, )\n",
    "\n",
    "        delta_L_hat = (deltas.T @ H[-1, :].T).reshape(-1, 1)\n",
    "        starting_point = combinations[combinations == 0]  # should always return 1 idx\n",
    "\n",
    "        # Normalize\n",
    "        vL_hat = np.zeros(L_hat.shape)\n",
    "        vL_hat[0, 0] = 1\n",
    "        for i in range(L_to_use.shape[1] - 1):\n",
    "            vL_hat[0, i + 1] = vL_hat[0, i] + delta_L_hat[i, 0]\n",
    "        L_hat[j, :] = np.divide(vL_hat, vL_hat[0, starting_point].item())\n",
    "\n",
    "    # Rank losses\n",
    "    indx = np.argmax(L_hat)\n",
    "    col = np.unique(combinations[0, indx])\n",
    "\n",
    "    # Update mcs\n",
    "    mcs[0, curr_set[0, col].astype(int)] = 0\n",
    "    removed = curr_set[0, col]\n",
    "    return mcs, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the MCS here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1337ea8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84eee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain which models are in the MCS here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
