{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d4ccb5",
   "metadata": {},
   "source": [
    "# Neural Network for Density Forecasting EC331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be40c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#######################################################################\n",
    "######################### Importing Packages ##########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.stats\n",
    "\n",
    "# need to clean these up later\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Activation, Input, Dense, LSTM, concatenate\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from numpy.random import rand\n",
    "from numpy import ix_\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc574d2",
   "metadata": {},
   "source": [
    "# Data Importing and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2169fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#######################################################################\n",
    "###################### Data Importing/Cleaning ########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# import data and calculate log returns from adjusted close\n",
    "df_Nikkei_RAW = pd.read_csv(\"C:/Warwick Final Year/RAE/Data/^N225.csv\")\n",
    "df_NASDAQ_RAW = pd.read_csv(\"C:/Warwick Final Year/RAE/Data/^IXIC.csv\")\n",
    "df_DAX_RAW = pd.read_csv(\"C:/Warwick Final Year/RAE/Data/^GDAXI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc572973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Processor(DATA,\n",
    "                   Batch_Size,\n",
    "                   Length_of_Batch,\n",
    "                   Test_Train_Split):\n",
    "    \n",
    "    DATA.columns = [c.replace(' ', '_') for c in DATA.columns]\n",
    "    DATA = DATA[DATA['Adj_Close'].notnull()]\n",
    "    DATA['log_ret'] = np.log(DATA.Adj_Close) - np.log(DATA.Adj_Close.shift(1))\n",
    "\n",
    "    # spilt to training and test sets\n",
    "    DATA    = DATA[['Date','log_ret']][1:]\n",
    "    DATA['Date'] = DATA['Date'].apply(pd.Timestamp)\n",
    "    DATA.set_index('Date',inplace=True, drop=True)\n",
    "    \n",
    "    train = DATA.loc[:Test_Train_Split]\n",
    "    test  = DATA.loc[Test_Train_Split:]\n",
    "   \n",
    "    DATA_train = [[i] for i in train['log_ret']]\n",
    "    DATA_test  = [[i] for i in test['log_ret']]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train)\n",
    "    scaled_train = scaler.transform(train)\n",
    "    scaled_test = scaler.transform(test)\n",
    "    \n",
    "    # creating time series generator for processing\n",
    "    # make one for training, one for validation\n",
    "    time_series_generator = TimeseriesGenerator(DATA_train, \n",
    "                                                DATA_train, \n",
    "                                                length=Length_of_Batch, \n",
    "                                                batch_size=Batch_Size)\n",
    "    time_series_val_generator = TimeseriesGenerator(DATA_test,\n",
    "                                                    DATA_test, \n",
    "                                                    length=Length_of_Batch, \n",
    "                                                    batch_size=Batch_Size)\n",
    "\n",
    "    \n",
    "    return {\"Data\" : DATA,\n",
    "            \"Training\" : time_series_generator,\n",
    "            \"Validation\" : time_series_val_generator}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_Size = 64\n",
    "Length     = 10\n",
    "Test_Train_Split = '2015-01-01'\n",
    "\n",
    "Nikkei = Data_Processor(df_Nikkei_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "NASDAQ = Data_Processor(df_NASDAQ_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "DAX = Data_Processor(df_DAX_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "\n",
    "DATA = {\n",
    "       \"Nikkei\"  : Nikkei,\n",
    "       \"NASDAQ\"  : NASDAQ,\n",
    "       \"DAX\"     : DAX\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3065e7",
   "metadata": {},
   "source": [
    "# Plotting Stock Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count,i in enumerate([Nikkei['Data'],\n",
    "         NASDAQ['Data'],\n",
    "         DAX['Data']]):\n",
    "    \n",
    "\n",
    "    if count == 0:\n",
    "        NAME = \"Nikkei\"\n",
    "    elif count == 1:\n",
    "        NAME = \"NASDAQ\"\n",
    "    else:\n",
    "        NAME = \"DAX\"\n",
    "    \n",
    "    ax = i.plot(y='log_ret', \n",
    "            kind='line',\n",
    "            rot=45,\n",
    "            legend=False,\n",
    "            title=NAME\n",
    "           )\n",
    "    \n",
    "    \n",
    "    means = np.repeat(0,len(i['log_ret']))\n",
    "    std   = np.repeat(np.std(i['log_ret']),len(i['log_ret']))\n",
    "    x     = pd.date_range(str(i.index.values[0]), str(i.index.values[-1]),freq=\"D\")\n",
    "    ax.axvline(pd.to_datetime('2015-01-01'),color='r',linestyle='--')\n",
    "    s1 = ax.fill_between(i.index.values, np.add(means,std),np.subtract(means,std), color='green',zorder=4,alpha=0.4)\n",
    "    s2 = ax.fill_between(i.index.values, np.add(means,np.multiply(2,std)),\n",
    "                     np.subtract(means,np.multiply(2,std)), \n",
    "                     color='grey',\n",
    "                     zorder=3,\n",
    "                     alpha=0.5)\n",
    "    ax.text(pd.to_datetime('2016-06-01'), \n",
    "            -0.18,\n",
    "        \"Test/Train Split\",\n",
    "        horizontalalignment='center', \n",
    "            fontweight='bold', \n",
    "            color='red', \n",
    "            rotation=-90,\n",
    "           fontsize='x-small')\n",
    "    \n",
    "    years = mdates.YearLocator(10)   # every year\n",
    "    years_fmt = mdates.DateFormatter('%Y')\n",
    "    \n",
    "    \n",
    "    ax.xaxis.set_major_locator(years)\n",
    "    ax.xaxis.set_major_formatter(years_fmt)\n",
    "    ax.set(ylabel = \"Log Returns\")\n",
    "    ax.legend(handles=[s1,s2], labels=[\"1 std\",\"2 std\"],loc='upper left')\n",
    "    ax.set_ylim([-0.2, 0.2])\n",
    "    plt.tight_layout() \n",
    "    ax.figure.savefig('C:/Warwick Final Year/RAE/Graphs/' + str(count) + 'logret.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb982d",
   "metadata": {},
   "source": [
    "# The Neural Network Architecture\n",
    "\n",
    "These neural network models work by assuming some parametrised distribution for the stock returns and forecasting the parameters of the distribution in the following period. \n",
    "\\\n",
    "As a loss function we use the log of the probability density function. Thus, our neural network is in effect being trained to converge to the maximum likelihood estimators. \n",
    "\\\n",
    "Basic building block is LSTM layers interspersed with dropout layers for regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74399194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Need to integrate loss function to model class, can't seem to get it to work consistently.\n",
    "Also, want to declutter and increase flexibility - i.e. not explicitly specifying depth and width in class,\n",
    "rather want to pick width and depth at instantiation. Could also add activation flexibility at instantiation too.\n",
    "\"\"\"\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, distribution):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.distribution = distribution\n",
    "        \n",
    "        # conditional mean channel\n",
    "        self.LSTM1 = tf.keras.layers.LSTM(16, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        self.LSTM2 = tf.keras.layers.LSTM(32, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        self.LSTM3 = tf.keras.layers.LSTM(16, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=False)\n",
    "        \n",
    "        \n",
    "        # scale/std channel \n",
    "        self.LSTM4 = tf.keras.layers.LSTM(16, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        self.LSTM5 = tf.keras.layers.LSTM(32, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        \n",
    "        self.LSTM6 = tf.keras.layers.LSTM(16, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=False)\n",
    "        \n",
    "        # DoF Channel\n",
    "        \n",
    "        self.LSTM7 = tf.keras.layers.LSTM(16, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        self.LSTM8 = tf.keras.layers.LSTM(32, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        \n",
    "        self.LSTM9 = tf.keras.layers.LSTM(16, \n",
    "                                          activation=tf.nn.relu,\n",
    "                                          return_sequences=False)\n",
    "\n",
    "\n",
    "        # dropout layers to regularise\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.4)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        self.dropout4 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout5 = tf.keras.layers.Dropout(0.4)\n",
    "        self.dropout6 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        self.dropout7 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout8 = tf.keras.layers.Dropout(0.4)\n",
    "        self.dropout9 = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "\n",
    "\n",
    "        # a dense layer for conditional mean\n",
    "        self.dense1 = tf.keras.layers.Dense(1, \n",
    "                                            activation='tanh')\n",
    "        # create a function to scale the sigmoid function to take values between 0 and 1/2 since a random\n",
    "        # variable taking values in [0,1] has std between these values (Popovinciu's Inequality)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, \n",
    "                                            activation='sigmoid')#lambda x: tf.keras.activations.sigmoid(x)*0.5)\n",
    "        \n",
    "        # for t-distribution the scale parameter doesn't correspond to std \n",
    "        # DoF >0 and scale >0 so use relu for these\n",
    "        \n",
    "        self.dense3 = tf.keras.layers.Dense(1,\n",
    "                                           activation='relu')\n",
    "        \n",
    "        self.dense4 = tf.keras.layers.Dense(1,\n",
    "                                           activation='relu')\n",
    "        \n",
    "    \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # LSTM --> Dropout --> dense with 2 outputs (conditional mean & std)\n",
    "        \n",
    "        if self.distribution in ['Normal',\"Laplace\"] :\n",
    "        \n",
    "            # mean channel\n",
    "            x1 = self.LSTM1(inputs)\n",
    "            x1 = self.dropout1(x1)    \n",
    "            x1 = self.LSTM2(x1)\n",
    "            x1 = self.dropout2(x1)\n",
    "            x1 = self.LSTM3(x1)\n",
    "            x1 = self.dropout3(x1)\n",
    "\n",
    "            # std channel\n",
    "            x2 = self.LSTM4(inputs)\n",
    "            x2 = self.dropout4(x2)\n",
    "            x2 = self.LSTM5(x2)\n",
    "            x2 = self.dropout5(x2)\n",
    "            x2 = self.LSTM6(x2)\n",
    "            x2 = self.dropout6(x2)\n",
    "\n",
    "            out1 = self.dense1(x1)\n",
    "            out2 = self.dense2(x2)\n",
    "\n",
    "            return concatenate([out1,out2])\n",
    "        \n",
    "        elif self.distribution == 't':\n",
    "            \n",
    "             # mean channel\n",
    "            x1 = self.LSTM1(inputs)\n",
    "            x1 = self.dropout1(x1)    \n",
    "            x1 = self.LSTM2(x1)\n",
    "            x1 = self.dropout2(x1)\n",
    "            x1 = self.LSTM3(x1)\n",
    "            x1 = self.dropout3(x1)\n",
    "\n",
    "            # scale channel\n",
    "            x2 = self.LSTM4(inputs)\n",
    "            x2 = self.dropout4(x2)\n",
    "            x2 = self.LSTM5(x2)\n",
    "            x2 = self.dropout5(x2)\n",
    "            x2 = self.LSTM6(x2)\n",
    "            x2 = self.dropout6(x2)\n",
    "            \n",
    "            # DoF channel for t dist\n",
    "            \n",
    "            x3 = self.LSTM7(inputs)\n",
    "            x3 = self.dropout7(x3)\n",
    "            x3 = self.LSTM8(x3)\n",
    "            x3 = self.dropout8(x3)\n",
    "            x3 = self.LSTM9(x3)\n",
    "            x3 = self.dropout9(x3)\n",
    "\n",
    "            out1 = self.dense1(x1)\n",
    "            out2 = self.dense3(x2)\n",
    "            out3 = self.dense4(x3)\n",
    "\n",
    "            return concatenate([out1,out2,out3])\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(distribution):\n",
    "    if distribution == \"Normal\":\n",
    "        def loss_comp(y_true, y_pred):\n",
    "            (mean_pred, scale_pred) = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "            dist = tfd.Normal(loc = mean_pred, scale=tf.math.add(scale_pred,1e-10))   \n",
    "            log_dens = tf.math.log(tf.math.add(dist.prob(y_true),1e-10))\n",
    "            loss = -tf.math.reduce_mean(log_dens)\n",
    "            return loss\n",
    "            \n",
    "    elif distribution == \"t\":\n",
    "        def loss_comp(y_true, y_pred):\n",
    "            (mean_pred, scale_pred, nu_pred) = tf.split(y_pred, num_or_size_splits=3, axis=1)\n",
    "            dist = tfd.StudentT(df = tf.math.add(nu_pred,1e-10), loc = mean_pred, scale=tf.math.add(scale_pred,1e-10))   \n",
    "            log_dens = tf.math.log(dist.prob(y_true))\n",
    "            loss = -tf.math.reduce_mean(log_dens)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "    elif distribution == \"Laplace\":\n",
    "        # in the dense output we use Popovinciu's inequality to get a bound on the \n",
    "        # variance / std. However, we the Laplacian variance is 2b^2 where b is the scale \n",
    "        # parameter. For our inequality to be valid we then \n",
    "        def loss_comp(y_true, y_pred):\n",
    "            (mean_pred, scale_pred) = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "            dist = tfd.Laplace(loc = mean_pred, \n",
    "                               scale=tf.math.add(scale_pred,1e-10))   \n",
    "            log_dens = tf.math.log(tf.math.add(dist.prob(y_true),1e-10))\n",
    "            loss = -tf.math.reduce_mean(log_dens)\n",
    "            return loss\n",
    "        \n",
    "    return loss_comp\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks\n",
    "\n",
    "path_checkpoint = \"C:/Warwick Final Year/RAE/Code Python/Model Checkpoints\"\n",
    "\n",
    "callbacks = {}\n",
    "for i in [\"Normal\",\"t\",\"Laplace\"]:\n",
    "\n",
    "    callbacks[i] = [ ModelCheckpoint(filepath=path_checkpoint + \"/\" + i,\n",
    "                                         monitor='val_loss',\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=True,\n",
    "                                         save_best_only=True),\n",
    "                    ReduceLROnPlateau(monitor='val_loss',\n",
    "                                              factor=0.1,\n",
    "                                              min_lr=1e-10,\n",
    "                                              patience=10,\n",
    "                                              verbose=1),\n",
    "                   EarlyStopping(monitor='loss', patience=50)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea60a7",
   "metadata": {},
   "source": [
    "# Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aefd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick which series to fit:\n",
    "\n",
    "which_series = \"NASDAQ\"\n",
    "\n",
    "if which_series not in [\"Nikkei\",\"NASDAQ\",\"DAX\"]:\n",
    "    raise Exception(\"That series is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4692f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_generator = DATA[which_series][\"Training\"]\n",
    "time_series_val_generator = DATA[which_series]['Validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00816747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Hyperparameters for training\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "learning_rates = {\n",
    "                  \"t\"       : 0.0001,\n",
    "                  \"Normal\"  : 0.01,\n",
    "                  \"Laplace\" : 0.001\n",
    "}\n",
    "\n",
    "loss_functions = {\n",
    "                  \"t\"       : loss_func(\"t\"),\n",
    "                  \"Normal\"  : loss_func(\"Normal\"),\n",
    "                  \"Laplace\" : loss_func(\"Laplace\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed as stochastic intialisation\n",
    "tf.random.set_seed(\n",
    "    11\n",
    ")\n",
    "\n",
    "# first we fit the model using a t-distribution\n",
    "model_t = Model(\"t\")\n",
    "\n",
    "model_t.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rates[\"t\"]),loss=loss_functions[\"t\"])\n",
    "\n",
    "\n",
    "t_History = model_t.fit(time_series_generator,        \n",
    "                        epochs = num_epochs, \n",
    "                        shuffle = False,\n",
    "                        validation_data=time_series_val_generator,\n",
    "                        callbacks=callbacks[\"t\"])\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_History = model_t.fit(time_series_generator,        \n",
    "                        epochs = num_epochs, \n",
    "                        shuffle = False,\n",
    "                        validation_data=time_series_val_generator,\n",
    "                        callbacks=callbacks[\"t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d74440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we fit the model with a normal distribution\n",
    "\n",
    "model_Normal = Model(\"Normal\")\n",
    "\n",
    "model_Normal.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),loss=loss_functions[\"Normal\"])\n",
    "\n",
    "\n",
    "Normal_History = model_Normal.fit(time_series_generator,        \n",
    "                                  epochs = num_epochs, \n",
    "                                  shuffle = False,\n",
    "                                  validation_data=time_series_val_generator,\n",
    "                                  callbacks=callbacks[\"Normal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e30349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally fit the model with a Laplace distribution\n",
    "\n",
    "model_Laplace = Model(\"Laplace\")\n",
    "\n",
    "model_Laplace.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=loss_functions[\"Laplace\"])\n",
    "\n",
    "Laplace_History = model_Laplace.fit(time_series_generator,        \n",
    "                                    epochs = num_epochs, \n",
    "                                    shuffle = False,\n",
    "                                    validation_data=time_series_val_generator,\n",
    "                                    callbacks=callbacks[\"Laplace\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439b740",
   "metadata": {},
   "source": [
    "# Plotting Forecasts\n",
    "Plot 5% and 10% VaR bands for each forecasted distribution and compared to observed stock returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd978f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_Pred_t     = model_t.predict(DATA[which_series]['Validation'])\n",
    "Val_Pred_Normal = model_Normal.predict(DATA[which_series]['Validation'])\n",
    "Val_Pred_Laplace = model_Laplace.predict(DATA[which_series]['Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set is from 2015-01-01, also need length of window forecasting over to predict first data point\n",
    "validation_data = DATA[which_series][\"Data\"].loc['2015-01-01':]['log_ret'][Length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9b99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first plot Normal\n",
    "means_N = [i[0] for i in Val_Pred_Normal]\n",
    "std_N   = [i[1] for i in Val_Pred_Normal]\n",
    "x = pd.date_range(\"2015-01-01\", periods=len(means_N),freq=\"D\")\n",
    "\n",
    "\n",
    "plt.plot(x,validation_data, color='#1f77b4',zorder=1)\n",
    "\n",
    "ppf = scipy.stats.norm.ppf\n",
    "SCORE = -round(Normal_History.history[\"val_loss\"][-1],4)\n",
    "NAME = \"Normal\"\n",
    "s1 = plt.fill_between(x,np.add(means_N,ppf(0.95,loc=means_N,scale=std_N)),np.add(means_N,ppf(0.05,loc=means_N,scale=std_N)), \n",
    "                      color='green',zorder=4,alpha=0.4)\n",
    "s2 = plt.fill_between(x,np.add(means_N,ppf(0.975,loc=means_N,scale=std_N)),\n",
    "                 np.add(means_N,ppf(0.025,loc=means_N,scale=std_N)), \n",
    "                 color='grey',\n",
    "                     zorder=3,\n",
    "                     alpha=0.5)\n",
    "plt.plot(x,means_N, color='black',zorder=2)\n",
    "\n",
    "plt.title(NAME)\n",
    "\n",
    "plt.text(pd.to_datetime('2019-06-01'), \n",
    "        -0.08,\n",
    "    \"SCORE=\"+str(SCORE),\n",
    "    horizontalalignment='center', \n",
    "        fontweight='bold', \n",
    "        color='red',\n",
    "       fontsize='medium')\n",
    "\n",
    "years = mdates.YearLocator(10)   # every year\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "\n",
    "#plt.xaxis.set_major_locator(years)\n",
    "#plt.xaxis.set_major_formatter(years_fmt)\n",
    "plt.ylabel(\"Log Returns\")\n",
    "plt.legend(handles=[s1,s2], labels=[\"10%\",\"5%\"],loc='upper right')\n",
    "plt.ylim([-0.15, 0.15])\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig('C:/Warwick Final Year/RAE/Graphs/' + which_series + '_Validation_' + NAME + '.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf657b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now t\n",
    "means_T = [i[0] for i in Val_Pred_t]\n",
    "scale_T   = [i[1] for i in Val_Pred_t]\n",
    "dfs_T   = [i[2] for i in Val_Pred_t]\n",
    "x = pd.date_range(\"2015-01-01\", periods=len(means_T),freq=\"D\")\n",
    "\n",
    "\n",
    "plt.plot(x,validation_data, color='#1f77b4',zorder=1)\n",
    "\n",
    "ppf = scipy.stats.t.ppf\n",
    "         \n",
    "SCORE = -round(t_History.history[\"val_loss\"][-1],4)\n",
    "NAME = \"Student_T\"\n",
    "         \n",
    "s1 = plt.fill_between(x,np.add(means_T,ppf(0.95,df=dfs_T, loc=means_T,scale=scale_T)),\n",
    "                      np.add(means_T,ppf(0.05,df=dfs_T,loc=means_T,scale=scale_T)), \n",
    "                      color='green',zorder=4,alpha=0.4)\n",
    "s2 = plt.fill_between(x,np.add(means_T,ppf(0.975,df=dfs_T,loc=means_T,scale=scale_T)),\n",
    "                 np.add(means_T,ppf(0.025,df=dfs_T,loc=means_T,scale=scale_T)), \n",
    "                 color='grey',\n",
    "                     zorder=3,\n",
    "                     alpha=0.5)\n",
    "plt.plot(x,means_T, color='black',zorder=2)\n",
    "\n",
    "plt.title(NAME)\n",
    "\n",
    "plt.text(pd.to_datetime('2019-06-01'), \n",
    "        -0.08,\n",
    "    \"SCORE=\"+str(SCORE),\n",
    "    horizontalalignment='center', \n",
    "        fontweight='bold', \n",
    "        color='red',\n",
    "       fontsize='medium')\n",
    "\n",
    "years = mdates.YearLocator(10)   # every year\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "\n",
    "#plt.xaxis.set_major_locator(years)\n",
    "#plt.xaxis.set_major_formatter(years_fmt)\n",
    "plt.ylabel(\"Log Returns\")\n",
    "plt.legend(handles=[s1,s2], labels=[\"10%\",\"5%\"],loc='upper right')\n",
    "plt.ylim([-0.15, 0.15])\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig('C:/Warwick Final Year/RAE/Graphs/' + which_series + '_Validation_' + 'Student_T' + '.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0981dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly the Laplacian\n",
    "means_L = [i[0] for i in Val_Pred_Laplace]\n",
    "scale_L   = [i[1] for i in Val_Pred_Laplace]\n",
    "x = pd.date_range(\"2015-01-01\", periods=len(means_L),freq=\"D\")\n",
    "\n",
    "\n",
    "plt.plot(x,validation_data, color='#1f77b4',zorder=1)\n",
    "\n",
    "ppf = scipy.stats.laplace.ppf\n",
    "         \n",
    "SCORE = -round(Laplace_History.history[\"val_loss\"][-1],4)\n",
    "NAME = \"Laplace\"\n",
    "         \n",
    "s1 = plt.fill_between(x,np.add(means_L,ppf(0.95,loc=means_L,scale=scale_L)),\n",
    "                      np.add(means_L,ppf(0.05,loc=means_L,scale=scale_L)), \n",
    "                      color='green',zorder=4,alpha=0.4)\n",
    "s2 = plt.fill_between(x,np.add(means_L,ppf(0.975,loc=means_L,scale=scale_L)),\n",
    "                 np.add(means_L,ppf(0.025,loc=means_L,scale=scale_L)), \n",
    "                 color='grey',\n",
    "                     zorder=3,\n",
    "                     alpha=0.5)\n",
    "plt.plot(x,means_L, color='black',zorder=2)\n",
    "\n",
    "plt.title(NAME)\n",
    "\n",
    "plt.text(pd.to_datetime('2019-06-01'), \n",
    "        -0.08,\n",
    "    \"SCORE=\"+str(SCORE),\n",
    "    horizontalalignment='center', \n",
    "        fontweight='bold', \n",
    "        color='red',\n",
    "       fontsize='medium')\n",
    "\n",
    "years = mdates.YearLocator(10)   # every year\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "\n",
    "#plt.xaxis.set_major_locator(years)\n",
    "#plt.xaxis.set_major_formatter(years_fmt)\n",
    "plt.ylabel(\"Log Returns\")\n",
    "plt.legend(handles=[s1,s2], labels=[\"10%\",\"5%\"],loc='upper right')\n",
    "plt.ylim([-0.15, 0.15])\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig('C:/Warwick Final Year/RAE/Graphs/' + which_series + '_Validation_' + NAME + '.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f224bac",
   "metadata": {},
   "source": [
    "# Exporting losses and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers \n",
    "\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "Normal_pdfs = scipy.stats.norm.pdf(\n",
    "                                   x=DAX['Data'].loc['2015-01-01':]['log_ret'][10:],\n",
    "                                   loc=means_N,\n",
    "                                   scale=std_N\n",
    "                                )\n",
    "\n",
    "T_pdfs = scipy.stats.t.pdf(\n",
    "                                   x=DAX['Data'].loc['2015-01-01':]['log_ret'][10:],\n",
    "                                   df=dfs_T,\n",
    "                                   loc=means_T,\n",
    "                                   scale=scale_T\n",
    "                                )\n",
    "\n",
    "Laplace_pdfs = scipy.stats.laplace.pdf(\n",
    "                                   x=DAX['Data'].loc['2015-01-01':]['log_ret'][10:],\n",
    "                                   loc=means_L,\n",
    "                                   scale=scale_L\n",
    "                                )\n",
    "\n",
    "Normal_L2 =[0 for i in range(len(means_N))]\n",
    "T_L2 = [0 for i in range(len(means_N))]\n",
    "Laplace_L2 =[0 for i in range(len(means_N))]\n",
    "\n",
    "for i in range(len(means_N)):\n",
    "    \n",
    "    Normal_L2[i] = np.power(integrate.quad(lambda x: np.power(scipy.stats.norm.pdf(x,\n",
    "                                                                              loc=means_N[i],\n",
    "                                                                              scale=std_N[i]),2),-np.inf,np.inf)[0] , 0.5)\n",
    "    \n",
    "    T_L2[i] = np.power(integrate.quad(lambda x: np.power(scipy.stats.t.pdf(x,\n",
    "                                                                      df=dfs_T[i],\n",
    "                                                                      loc=means_T[i],\n",
    "                                                                      scale=scale_T[i]),2),-np.inf,np.inf)[0] , 0.5)\n",
    "    \n",
    "    Laplace_L2[i] = np.power(integrate.quad(lambda x: np.power(scipy.stats.laplace.pdf(x,\n",
    "                                                                               loc=means_L[i],\n",
    "                                                                               scale=scale_L[i]),2),-np.inf,np.inf)[0] , 0.5)\n",
    "\n",
    "\n",
    "# get the series of validation log losses for the three models\n",
    "\n",
    "T_losses_log       =  -np.log(T_pdfs\n",
    "                         )\n",
    "\n",
    "Normal_losses_log  = -np.log(Normal_pdfs\n",
    "                        )\n",
    "\n",
    "Laplace_losses_log = -np.log(Laplace_pdfs\n",
    "                        )\n",
    "\n",
    "# also the quad losses\n",
    "\n",
    "Normal_losses_Quad = -(np.multiply(Normal_pdfs,2)-\n",
    "                                  np.power(Normal_L2,2))\n",
    "            \n",
    "Laplace_losses_Quad = -(np.multiply(Laplace_pdfs,2)-\n",
    "                      np.power(Laplace_L2,2))\n",
    "\n",
    "\n",
    "T_losses_Quad = -(np.multiply(T_pdfs,2)-\n",
    "                      np.power(T_L2,2))\n",
    "#finally the spherical scores\n",
    "\n",
    "Normal_losses_Sph = -np.divide(Normal_pdfs,Normal_L2)\n",
    "\n",
    "Laplace_losses_Sph = -np.divide(Laplace_pdfs,Laplace_L2)\n",
    "\n",
    "T_losses_Sph = -np.divide(T_pdfs,T_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'Student_T_losses_log' : T_losses_log,\n",
    "    'Normal_losses_log'    : Normal_losses_log,\n",
    "    'Laplace_losses_log'   : Laplace_losses_log,\n",
    "    'Student_T_losses_quad' : T_losses_Quad,\n",
    "    'Normal_losses_quad'    : Normal_losses_Quad,\n",
    "    'Laplace_losses_quad'   : Laplace_losses_Quad,\n",
    "    'Student_T_losses_sph' : T_losses_Sph,\n",
    "    'Normal_losses_sph'    : Normal_losses_Sph,\n",
    "    'Laplace_losses_sph'   : Laplace_losses_Sph,\n",
    "    'Normal_loc'       : means_N,\n",
    "    'Normal_scale'     : std_N,\n",
    "    'Student_T_loc'    : means_T,\n",
    "    'Student_T_scale'  : scale_T,\n",
    "    'Student_T_shape'  : dfs_T,\n",
    "    'Laplace_loc'      : means_L,\n",
    "    'Laplace_scale'    : scale_L\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0179f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Warwick Final Year/RAE/Processed Data/' + which_series + '_predictions_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a2a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
