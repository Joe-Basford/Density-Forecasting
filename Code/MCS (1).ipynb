{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8eb76bf",
   "metadata": {},
   "source": [
    "# Creating the MCS (Hansen, Lunde, Nason ; 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ad1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#######################################################################\n",
    "######################### Importing Packages ##########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.stats\n",
    "\n",
    "# need to clean these up later\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Activation, Input, Dense, LSTM, concatenate\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from numpy.random import rand\n",
    "from numpy import ix_\n",
    "import os\n",
    "import collections\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df900fd1",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc461542",
   "metadata": {},
   "source": [
    "## ARMA-GARCH Importing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcd00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Import Nikkei, DAX, and NASDAQ data\n",
    "def Data_Processor(DATA,\n",
    "                   Batch_Size,\n",
    "                   Length_of_Batch,\n",
    "                   Test_Train_Split):\n",
    "    \n",
    "    DATA.columns = [c.replace(' ', '_') for c in DATA.columns]\n",
    "    DATA = DATA[DATA['Adj_Close'].notnull()]\n",
    "    DATA['log_ret'] = np.log(DATA.Adj_Close) - np.log(DATA.Adj_Close.shift(1))\n",
    "\n",
    "    # spilt to training and test sets\n",
    "    DATA    = DATA[['Date','log_ret']][1:]\n",
    "    DATA['Date'] = DATA['Date'].apply(pd.Timestamp)\n",
    "    DATA.set_index('Date',inplace=True, drop=True)\n",
    "    \n",
    "    train = DATA.loc[:Test_Train_Split]\n",
    "    test  = DATA.loc[Test_Train_Split:]\n",
    "   \n",
    "    DATA_train = [[i] for i in train['log_ret']]\n",
    "    DATA_test  = [[i] for i in test['log_ret']]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train)\n",
    "    scaled_train = scaler.transform(train)\n",
    "    scaled_test = scaler.transform(test)\n",
    "    \n",
    "    # creating time series generator for processing\n",
    "    # make one for training, one for validation\n",
    "    time_series_generator = TimeseriesGenerator(DATA_train, \n",
    "                                                DATA_train, \n",
    "                                                length=Length_of_Batch, \n",
    "                                                batch_size=Batch_Size)\n",
    "    time_series_val_generator = TimeseriesGenerator(DATA_test,\n",
    "                                                    DATA_test, \n",
    "                                                    length=Length_of_Batch, \n",
    "                                                    batch_size=Batch_Size)\n",
    "\n",
    "    \n",
    "    return {\"Data\" : DATA,\n",
    "            \"Training\" : time_series_generator,\n",
    "            \"Validation\" : time_series_val_generator}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ece64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9bedbba0040e>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DATA['log_ret'] = np.log(DATA.Adj_Close) - np.log(DATA.Adj_Close.shift(1))\n",
      "<ipython-input-2-9bedbba0040e>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DATA['log_ret'] = np.log(DATA.Adj_Close) - np.log(DATA.Adj_Close.shift(1))\n"
     ]
    }
   ],
   "source": [
    "df_Nikkei_RAW = pd.read_csv(\"C:/Warwick Final Year/RAE/Data/^N225.csv\")\n",
    "df_NASDAQ_RAW = pd.read_csv(\"C:/Warwick Final Year/RAE/Data/^IXIC.csv\")\n",
    "df_DAX_RAW = pd.read_csv(\"C:/Warwick Final Year/RAE/Data/^GDAXI.csv\")\n",
    "\n",
    "Batch_Size = 64\n",
    "Length = 10\n",
    "Test_Train_Split = '2015-01-01'\n",
    "\n",
    "\n",
    "Nikkei = Data_Processor(df_Nikkei_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "NASDAQ = Data_Processor(df_NASDAQ_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "DAX = Data_Processor(df_DAX_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "\n",
    "DATA = {'Nikkei' : Nikkei,\n",
    "        'NASDAQ' : NASDAQ,\n",
    "        'DAX'    : DAX}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07072ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ARMA-GARCH models.\n",
    "\n",
    "data_path = \"C:/Warwick Final Year/RAE/Processed Data/\"\n",
    "\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "def loss_series_calculator_ARMA_GARCH(which_series):\n",
    "    \n",
    "    if which_series   == 'Nikkei':\n",
    "        SERIES = Nikkei\n",
    "    elif which_series == 'NASDAQ':\n",
    "        SERIES = NASDAQ\n",
    "    elif which_series == 'DAX':\n",
    "        SERIES = DAX\n",
    "    else:\n",
    "        raise Exception(\"Sorry, which_series must be Nikkei, NASDAQ, or DAX\")\n",
    "    \n",
    "    \n",
    "    dict_of_losses = collections.defaultdict(lambda : collections.defaultdict(dict))\n",
    "    \n",
    "    \n",
    "    for filename in os.listdir(data_path + which_series +'/ARMA_GARCH'):\n",
    "        if filename.endswith('.csv'):\n",
    "            \n",
    "            df = pd.read_csv(data_path + \n",
    "                             which_series +\n",
    "                             '/ARMA_GARCH/' + \n",
    "                             filename)\n",
    "            \n",
    "            Normal_pdfs = scipy.stats.norm.pdf(SERIES['Data'].loc['2015-01-01':]['log_ret'],\n",
    "                                         loc = df['Norm_mu'],\n",
    "                                         scale = df['Norm_std'])\n",
    "            \n",
    "            Laplace_pdfs = scipy.stats.laplace.pdf(SERIES['Data'].loc['2015-01-01':]['log_ret'],\n",
    "                                         loc = df['Lap_mu'],\n",
    "                                         scale = np.divide(df['Lap_std'],np.sqrt(2)))\n",
    "            \n",
    "            T_pdfs = scipy.stats.t.pdf(SERIES['Data'].loc['2015-01-01':]['log_ret'],\n",
    "                                         df= df['T_shape'],\n",
    "                                         loc = df['T_mu'],\n",
    "                                         scale = np.divide(df['T_std'],np.sqrt(df['T_shape']/(df['T_shape']-2))))\n",
    "            \n",
    "            means_N = df['Norm_mu']\n",
    "            std_N = df['Norm_std']\n",
    "            \n",
    "            dfs_T = df['T_shape']\n",
    "            means_T = df['T_mu']\n",
    "            scale_T = np.divide(df['T_std'],np.sqrt(df['T_shape']/(df['T_shape']-2)))\n",
    "            \n",
    "            means_L = df['Lap_mu']\n",
    "            scale_L = np.divide(df['Lap_std'],np.sqrt(2))\n",
    "            \n",
    "            Normal_L2 =[0 for i in range(len(df['Norm_mu']))]\n",
    "            T_L2 = [0 for i in range(len(df['Norm_mu']))]\n",
    "            Laplace_L2 =[0 for i in range(len(df['Norm_mu']))]\n",
    "\n",
    "            for i in range(len(T_L2)):\n",
    "                \n",
    "                # basic calculations give L2 norms for each of the pdfs:\n",
    "                \n",
    "                Normal_L2[i] = np.power(1/(2*std_N[i]*np.sqrt(math.pi)) , 0.5)\n",
    "                \n",
    "                T_L2[i] = np.power(integrate.quad(lambda x: np.power(scipy.stats.t.pdf(x,\n",
    "                                                                                  df=dfs_T[i],\n",
    "                                                                                  loc=means_T[i],\n",
    "                                                                                  scale=scale_T[i]),2),-np.inf,np.inf)[0] , 0.5)\n",
    "                \n",
    "                Laplace_L2[i] = np.power(1/(4*scale_L[i]) , 0.5)\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "           # get the series of validation log losses for the three models\n",
    "\n",
    "            T_losses_log       =  -np.log(T_pdfs\n",
    "                                     )\n",
    "\n",
    "            Normal_losses_log  = -np.log(Normal_pdfs\n",
    "                                    )\n",
    "\n",
    "            Laplace_losses_log = -np.log(Laplace_pdfs\n",
    "                                    )\n",
    "\n",
    "            # also the quad losses\n",
    "\n",
    "            Normal_losses_Quad = -(np.multiply(Normal_pdfs,2)-\n",
    "                                              np.power(Normal_L2,2))\n",
    "\n",
    "            Laplace_losses_Quad = -(np.multiply(Laplace_pdfs,2)-\n",
    "                                  np.power(Laplace_L2,2))\n",
    "\n",
    "\n",
    "            T_losses_Quad = -(np.multiply(T_pdfs,2)-\n",
    "                                  np.power(T_L2,2))\n",
    "            #finally the spherical scores\n",
    "\n",
    "            Normal_losses_Sph = -np.divide(Normal_pdfs,Normal_L2)\n",
    "\n",
    "            Laplace_losses_Sph = -np.divide(Laplace_pdfs,Laplace_L2)\n",
    "\n",
    "            T_losses_Sph = -np.divide(T_pdfs,T_L2)\n",
    "            \n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['Normal']['log'] = Normal_losses_log\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['Laplace']['log'] = Laplace_losses_log\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['T']['log'] = T_losses_log\n",
    "            \n",
    "            #\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['Normal']['quad'] = Normal_losses_Quad\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['Laplace']['quad'] = Laplace_losses_Quad\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['T']['quad'] = T_losses_Quad\n",
    "            \n",
    "            #\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['Normal']['sph'] = Normal_losses_Sph\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['Laplace']['sph'] = Laplace_losses_Sph\n",
    "            \n",
    "            dict_of_losses[filename[:-4 or None]]['T']['sph'] = T_losses_Sph\n",
    "            \n",
    "            print(filename + \" for \"+ which_series +\" is done!\")\n",
    "            \n",
    "            \n",
    "    return dict_of_losses\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab162e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMA_11_GARCH_11.csv for DAX is done!\n",
      "ARMA_22_GARCH_22.csv for DAX is done!\n",
      "ARMA_33_GARCH_33.csv for DAX is done!\n",
      "ARMA_44_GARCH_44.csv for DAX is done!\n",
      "ARMA_55_GARCH_55.csv for DAX is done!\n",
      "ARMA_11_GARCH_11.csv for NASDAQ is done!\n",
      "ARMA_22_GARCH_22.csv for NASDAQ is done!\n",
      "ARMA_33_GARCH_33.csv for NASDAQ is done!\n",
      "ARMA_44_GARCH_44.csv for NASDAQ is done!\n",
      "ARMA_55_GARCH_55.csv for NASDAQ is done!\n",
      "ARMA_11_GARCH_11.csv for Nikkei is done!\n",
      "ARMA_22_GARCH_22.csv for Nikkei is done!\n",
      "ARMA_33_GARCH_33.csv for Nikkei is done!\n",
      "ARMA_44_GARCH_44.csv for Nikkei is done!\n",
      "ARMA_55_GARCH_55.csv for Nikkei is done!\n"
     ]
    }
   ],
   "source": [
    "# this takes ages as I couldn't work out a closed form solution for the L2 norm of the t-pdf\n",
    "# also asked chat GPT and it said it didn't have one!\n",
    "ARMA_GARCH = {series : loss_series_calculator_ARMA_GARCH(series) for series in ['DAX','NASDAQ','Nikkei']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6e03c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.96953079, -0.5699212 , -6.12756825, ..., -7.16684605,\n",
       "       -5.79341307, -7.60634306])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of use:\n",
    "\n",
    "ARMA_GARCH['DAX']['ARMA_33_GARCH_33']['Normal']['sph']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4b62e",
   "metadata": {},
   "source": [
    "## Neural Network Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a2c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nikkei_NN = pd.read_csv(data_path + \"Nikkei/Neural_Network/Nikkei_predictions_losses.csv\")\n",
    "NASDAQ_NN = pd.read_csv(data_path + \"NASDAQ/Neural_Network/NASDAQ_predictions_losses.csv\")\n",
    "DAX_NN = pd.read_csv(data_path + \"DAX/Neural_Network/DAX_predictions_losses.csv\")\n",
    "\n",
    "\n",
    "NN = {'Nikkei' : Nikkei_NN,\n",
    "      'NASDAQ' : NASDAQ_NN,\n",
    "      'DAX'    : DAX_NN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bc947",
   "metadata": {},
   "source": [
    "# The MCS \n",
    "\n",
    "Code is taken from github repo https://github.com/ogrnz/feval\n",
    "\n",
    "Method is based on initial work by Hansen, Lunde, and Nason (2011)\n",
    "\n",
    "The MCS methodology works off of a test ($\\delta $) and an elimination rule ($e$)\n",
    "\n",
    "As test we use the multivariate Giacomini-White test of Borup et al. (2022)\n",
    "\n",
    "As elimination rule we remove the model with the highest average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb6d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Literal, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import arch.covariance.kernel as kernels\n",
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "def gw(L,\n",
    "       tau,\n",
    "       H,\n",
    "       kernele,\n",
    "       bw,\n",
    "       kernel_kwargs,\n",
    "       alpha=0.5):\n",
    "    \"\"\"\n",
    "    Test of Equal Conditional Predictive Ability by Giacomini and White (2006).\n",
    "    Used here for testing and debugging but made available through the package interface.\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Giacomini (https://gist.github.com/ogrnz/91f37140011d1c2447934766274c4070)\n",
    "    References:\n",
    "        - Giacomini, R., & White, H. (2006). Tests of conditional predictive ability. Econometrica, 74(6), 1545-1578.\n",
    "    :param L: (Tx2) array of forecast losses\n",
    "    :param H: (Txq) array of instruments. If `None` provided, defaults to the unconditional EPA (DM test)\n",
    "    :param tau: Forecast horizon\n",
    "    :param kernel: (default `None`)\n",
    "        If multistep forecast (`tau` > 1), covariance matrix is an HAC estimator.\n",
    "        Original implementation uses a Bartlett kernel with bandwidth `tau - 1`.\n",
    "        If a `str`, must match one of `arch` package variance estimator:\n",
    "         > https://arch.readthedocs.io/en/latest/covariance/covariance.html\n",
    "        If a `Callable`, must simply return a (qxq) covariance matrix (see arg `H`).\n",
    "    :param bw: (default `None`)\n",
    "        Bandwidth of the `kernel`. Typically set to `forecasting horizon - 1`.\n",
    "        If set to `None`, will let the kernel compute the optimal bandwidth if supported.\n",
    "    :param kernel_kwargs: (default `None`)\n",
    "        An optional dict of `argument: value` passed to `kernel`.\n",
    "        If `kernel` is a `Callable`, the eventual bandwidth must be passed here.\n",
    "    :param alpha: Significance level\n",
    "    :return: tuple(S, crit_val, p_val)\n",
    "        S: test statistic,\n",
    "        cval: critical value for significance lvl,\n",
    "        pval: p-value of test\n",
    "    \"\"\"\n",
    "    T = L.shape[0]  # Number of observations\n",
    "    d = L[:, 0] - L[:, 1]  # Loss differential\n",
    "    if H is None:  # Instruments (defaults to unconditional EPA)\n",
    "        H = np.ones((T, 1))\n",
    "    q = H.shape[1]\n",
    "\n",
    "    reg = np.empty(H.shape)\n",
    "    for jj in range(H.shape[1]):\n",
    "        reg[:, jj] = H[:, jj] * d\n",
    "\n",
    "    if tau == 1:  # One-step\n",
    "        beta = np.linalg.lstsq(reg, np.ones(T), rcond=None)[0][0].item()\n",
    "        res = np.ones(T) - beta * reg\n",
    "        r2 = 1 - np.mean(res ** 2)\n",
    "        S = T * r2\n",
    "    else:  # Multistep\n",
    "        omega = np.empty((q, q))  # Defined here to make linter happy and inform about dims\n",
    "        if isinstance(kernel, Callable):  # Custom callable\n",
    "            omega = kernel(reg, **kernel_kwargs)\n",
    "        elif isinstance(kernel, str):  # Arch covariance\n",
    "            kerfunc = getattr(kernels, kernel)\n",
    "            ker = kerfunc(reg, bandwidth=bw, **kernel_kwargs)\n",
    "            omega = ker.cov.long_run\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        zbar = reg.mean().T\n",
    "        S = (T * zbar.T * np.linalg.pinv(omega) * zbar).item()\n",
    "\n",
    "    dof = reg.shape[1]\n",
    "    cval = chi2.ppf(1 - alpha, dof)\n",
    "    pval = 1 - chi2.cdf(abs(S), dof)\n",
    "\n",
    "    return S, cval, pval\n",
    "\n",
    "\n",
    "def mgw(L: np.array,\n",
    "        H: Optional = None,\n",
    "        covar_style: Literal[\"sample\", \"hac\"] = \"sample\",\n",
    "        kernel: Optional[Union[str, Callable]] = None,\n",
    "        bw: Optional[int] = None,\n",
    "        kernel_kwargs: Optional[dict] = None,\n",
    "        alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Implements the multivariate Giacomini-White (MGW) (Borup et al., 2022) test of equal predictive ability.\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Borup (https://sites.google.com/view/danielborup/research)\n",
    "    Notes:\n",
    "        If only 2 models are compared, it reduces to the Giacomini-White test (GW) (Giacomini and White, 2006)\n",
    "        If further no conditioning information H is given, it reduces to the\n",
    "        original Diebold-Mariano test (DM) (Diebold and Mariano, 1995)\n",
    "        If more than 2 models are compared but with no conditioning information H,\n",
    "        it reduces to multivariate Diebold-Mariano (MDM) (Mariano and Preve, 2012)\n",
    "    References:\n",
    "        - Borup, Daniel and Eriksen, Jonas Nygaard and Kjær, Mads Markvart and Thyrsgaard, Martin,\n",
    "        Predicting Bond Return Predictability. Available at http://dx.doi.org/10.2139/ssrn.3513340\n",
    "        - Diebold, F.X., and R.S. Mariano (1995) ‘Comparing Predictive Accuracy,’ Journal\n",
    "        of Business and Economic Statistics 13, 253–263.\n",
    "        - Giacomini, R., & White, H. (2006). Tests of conditional predictive ability.\n",
    "        Econometrica, 74(6), 1545-1578.\n",
    "        - Mariano, R. S., & Preve, D. (2012). Statistical tests for multiple forecast comparison.\n",
    "        Journal of econometrics, 169(1), 123-130.\n",
    "    :param L:\n",
    "        Txk matrix of losses of k models with T forecasts.\n",
    "    :param H: (default `None`)\n",
    "        Txq matrix of a constant and information set (test function).\n",
    "        If not provided, set to a (Tx1) column vector of 1, amounts to the\n",
    "        unconditional MWG test, which is equivalent to the multivariate Diebold-Mariano (Mariano and Preve, 2012).\n",
    "    :param covar_style: (default 'sample')\n",
    "        How to compute the covariance matrix.\n",
    "        Either the sample covariance ('sample') or an HAC estimator ('hac').\n",
    "    :param kernel: (default `None`)\n",
    "        If covariance matrix is an HAC estimator, what type to compute.\n",
    "        If a `str`, must match one of `arch` package variance estimator:\n",
    "         > https://arch.readthedocs.io/en/latest/covariance/covariance.html\n",
    "        If a `Callable`, must simply return a\n",
    "    :param bw: (default `None`)\n",
    "        Bandwidth of the `kernel`. Typically set to `forecasting horizon - 1`.\n",
    "        If set to `None`, will let the kernel compute the optimal bandwidth if supported.\n",
    "    :param kernel_kwargs: (default `None`)\n",
    "        An optional dict of `argument: value` passed to `kernel`.\n",
    "        If `kernel` is a `Callable`, the eventual bandwidth must be passed here.\n",
    "    :param alpha: (default 0.05)\n",
    "        Significance level.\n",
    "    :return: tuple(S, cval, pval)\n",
    "        S: float, the computed test statistic\n",
    "        cval: float, the corresponding critical value\n",
    "        pval: float, the p-value of S.\n",
    "    \"\"\"\n",
    "    # Args checks\n",
    "    if kernel is not None and covar_style == \"sample\":\n",
    "        raise ValueError(f\"{kernel=} incompatible with {covar_style=}.\")\n",
    "    if kernel is None and covar_style == \"hac\":\n",
    "        raise ValueError(\"Set `kernel` when using an HAC estimator.\")\n",
    "    if bw is not None and covar_style == \"sample\":\n",
    "        raise ValueError(f\"{bw=} incompatible with {covar_style=}.\")\n",
    "    if L.shape[1] < 2:\n",
    "        raise ValueError(f\"Not enough columns for matrix of  losses {L.shape[1]=}.\")\n",
    "\n",
    "    if kernel_kwargs is None:\n",
    "        kernel_kwargs = {}\n",
    "\n",
    "    # Initialize\n",
    "    T = L.shape[0]\n",
    "    p = L.shape[1] - 1\n",
    "    if H is None:  # defaults to unconditional EPA\n",
    "        H = np.ones((T, 1))\n",
    "\n",
    "    # Loss differentials\n",
    "    D = np.diff(L, axis=1)\n",
    "\n",
    "    # Conditioning information\n",
    "    q = H.shape[1]\n",
    "    reg = np.empty((T, q * p))\n",
    "\n",
    "    for i in range(T):\n",
    "        reg[i, :] = np.kron(H[i, :], D[i, :])\n",
    "\n",
    "    Dbar = np.mean(reg, axis=0)\n",
    "   \n",
    "    # Compute covar matrix\n",
    "    omega = np.empty((q * p, q * p))  # Defined here to make linter happy and inform about dims\n",
    "    if covar_style == \"sample\":\n",
    "        omega = (reg - Dbar).T @ (reg - Dbar) / (T - 1)\n",
    "    elif covar_style == \"hac\":  # HAC estimator\n",
    "        if isinstance(kernel, Callable):  # Custom callable\n",
    "            omega = kernel(reg, **kernel_kwargs)\n",
    "        elif isinstance(kernel, str):  # Arch covariance\n",
    "            kerfunc = getattr(kernels, kernel)\n",
    "          \n",
    "            ker = kerfunc(reg, bandwidth=bw, **kernel_kwargs)\n",
    "            omega = ker.cov.long_run\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Compute statistic\n",
    "    dof = q * p\n",
    "    S = (T * Dbar @ np.linalg.pinv(omega) @ Dbar.T).item()\n",
    "    cval = chi2.ppf(1 - alpha, dof)\n",
    "    pval = 1 - chi2.cdf(S, dof)\n",
    "\n",
    "    return S, cval, pval\n",
    "\n",
    "\n",
    "def cmcs(L: np.array, H: Optional = None, alpha: float = 0.05, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform the Conditional Model Confidence Set (CMCS).\n",
    "    The MCS procedure from Hansen (2011) is adapted to use MGW (Borup et al., 2022)\n",
    "    instead of bootstrapping the critical values. Allows to reduce an initial set of models to a\n",
    "    set of models with equal (conditional) predictive ability.\n",
    "    Also, allows to use conditioning information (`H`, hence the 'Conditional'),\n",
    "    to get the best MCS based on expected future loss.\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Borup (https://sites.google.com/view/danielborup/research)\n",
    "    References:\n",
    "        - Borup, Daniel and Eriksen, Jonas Nygaard and Kjær, Mads Markvart and Thyrsgaard, Martin,\n",
    "        Predicting Bond Return Predictability. Available at http://dx.doi.org/10.2139/ssrn.3513340\n",
    "        - Hansen, P. R., Lunde, A., & Nason, J. M. (2011). The model confidence set. Econometrica, 79(2), 453-497.\n",
    "    :param L:\n",
    "        (Txk) matrix of losses of k models with T forecasts.\n",
    "    :param H: (default `None`)\n",
    "        (Txq) matrix of a constant and information set (test function).\n",
    "        If not provided, set to a (Tx1) column vector of 1, amounts to the\n",
    "        unconditional MWG test, which is equivalent to the multivariate Diebold-Mariano (Mariano and Preve, 2012).\n",
    "    :param alpha: (default 0.05)\n",
    "        Significance level used in the MGW test.\n",
    "    :param **kwargs: Arguments passed to `feval.mgw`. Usually define covariance estimator and such.\n",
    "    :return: tuple(mcs, S, cval, pval, removed)\n",
    "        mcs: (1xk) np.array where models included in the best model confidence set are noted as 1.\n",
    "        S: float, the computed test statistic of the last test.\n",
    "        cval: float, the corresponding critical value.\n",
    "        pval: float, the p-value of S.\n",
    "        removed: (1xk) np.array where a column represents an algorithm cycle.\n",
    "            That way, we can see which model index was removed at which iteration.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    T = L.shape[0]\n",
    "    k = L.shape[1]\n",
    "    if H is None:\n",
    "        H = np.ones((T, 1))\n",
    "\n",
    "    # Init loop\n",
    "    S, cval, pval = np.inf, 1, 1\n",
    "    mcs = np.ones((1, k))\n",
    "    removed = np.zeros((1, k))\n",
    "\n",
    "    j = 0\n",
    "    while S > cval:\n",
    "        # Create L_to_use, the losses of models still in MCS\n",
    "        L_to_use = L[:, (mcs == 1)[0]]\n",
    "\n",
    "        if L_to_use.shape[1] == 1:  # Only 1 model left in set\n",
    "            break\n",
    "\n",
    "        # Perform MGW\n",
    "        S, cval, pval = mgw(L_to_use, H, alpha=alpha, **kwargs)\n",
    "\n",
    "        # H0 still rejected, apply elimination criterion\n",
    "        if S > cval:\n",
    "            mcs, removed[0, j] = elim_rule(L, mcs, H)\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    return mcs, S, cval, pval, removed\n",
    "\n",
    "\n",
    "def elim_rule(L: np.array,\n",
    "              mcs: np.array,\n",
    "              H: Optional = None):\n",
    "    \"\"\"\n",
    "    Elimination rule that allows to rank losses based on expected future loss given the information set `H`.\n",
    "    If `H` is a vector of constant, it amounts to ranking losses based on average loss.\n",
    "    See Borup et al. (2022) and Hansen (2011).\n",
    "    This is a reimplementation from the MATLAB code provided by\n",
    "    Borup (https://sites.google.com/view/danielborup/research)\n",
    "    References:\n",
    "        - Borup, Daniel and Eriksen, Jonas Nygaard and Kjær, Mads Markvart and Thyrsgaard, Martin,\n",
    "        Predicting Bond Return Predictability. Available at http://dx.doi.org/10.2139/ssrn.3513340\n",
    "        - Hansen, P. R., Lunde, A., & Nason, J. M. (2011). The model confidence set. Econometrica, 79(2), 453-497.\n",
    "    :param L:\n",
    "        (Txk) matrix of losses of k models with T forecasts.\n",
    "    :param mcs:\n",
    "        (1xk) vector of current model confidence set, where the least performing model will be eliminated.\n",
    "    :param H: (default `None`)\n",
    "        (Txq) matrix of a constant and information set (test function).\n",
    "    :return: tuple(mcs, removed)\n",
    "        mcs: (1xk) np.array where models included in the best model confidence set are noted as 1.\n",
    "        removed: (1xk) np.array where a column represents an algorithm cycle.\n",
    "            That way, we can see which model index was removed at which iteration.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    k = mcs.shape[1]\n",
    "    q = H.shape[1]\n",
    "    new_k = np.count_nonzero(mcs)\n",
    "\n",
    "    if L.shape[1] != k:\n",
    "        raise ValueError(f\"Dimensions of {L.shape[1]=} do not match {mcs.shape[1]=}.\")\n",
    "\n",
    "    L_to_use = np.zeros((L.shape[0], new_k))\n",
    "    curr_set = np.zeros((1, new_k))\n",
    "    j = 0\n",
    "    for i in range(k):  # TODO could be simplified?\n",
    "        if mcs[0, i] == 1:\n",
    "            L_to_use[:, j] = L[:, i]\n",
    "            curr_set[0, j] = i\n",
    "            j += 1\n",
    "\n",
    "    combinations = np.arange(0, j).reshape(1, -1)  # TODO why matrix? could be vect\n",
    "    L_hat = np.zeros(combinations.shape)\n",
    "\n",
    "    # Estimate\n",
    "    for j in range(combinations.shape[0]):  # TODO no loop if vect\n",
    "        L_intra_use = L_to_use[:, combinations[j, :]]  # TODO directly call j?\n",
    "\n",
    "        deltas = np.zeros((q, new_k - 1))\n",
    "        for i in range(L_to_use.shape[1] - 1):\n",
    "            Y_used = L_intra_use[:, i + 1] - L_intra_use[:, i]\n",
    "            Y_used = Y_used.reshape(-1, 1)\n",
    "            deltas[:, i] = (np.linalg.inv(H.T @ H) @ H.T @ Y_used).reshape(-1, )\n",
    "\n",
    "        delta_L_hat = (deltas.T @ H[-1, :].T).reshape(-1, 1)\n",
    "        starting_point = combinations[combinations == 0]  # should always return 1 idx\n",
    "\n",
    "        # Normalize\n",
    "        vL_hat = np.zeros(L_hat.shape)\n",
    "        vL_hat[0, 0] = 1\n",
    "        for i in range(L_to_use.shape[1] - 1):\n",
    "            vL_hat[0, i + 1] = vL_hat[0, i] + delta_L_hat[i, 0]\n",
    "        L_hat[j, :] = np.divide(vL_hat, vL_hat[0, starting_point].item())\n",
    "\n",
    "    # Rank losses\n",
    "    indx = np.argmax(L_hat)\n",
    "    col = np.unique(combinations[0, indx])\n",
    "\n",
    "    # Update mcs\n",
    "    mcs[0, curr_set[0, col].astype(int)] = 0\n",
    "    removed = curr_set[0, col]\n",
    "    return mcs, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d794172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the MCS here\n",
    "\n",
    "def MCS_calculator(which_series,\n",
    "             ARMA_losses,\n",
    "              NN_Data,\n",
    "                   loss,\n",
    "                  alpha):\n",
    "    \n",
    "    T = len(NN_Data[which_series]['Student_T_losses_'+loss])\n",
    "    \n",
    "    L = np.vstack([\n",
    "                          NN_Data[which_series]['Student_T_losses_'+loss],\n",
    "                          NN_Data[which_series]['Laplace_losses_'+loss],\n",
    "                          NN_Data[which_series]['Normal_losses_'+loss]]\n",
    "                         )\n",
    "    \n",
    "    \n",
    "    for k in ARMA_losses[which_series].keys(): \n",
    "        for dist in ['T','Normal','Laplace']:\n",
    "\n",
    "            L = np.vstack([L, ARMA_losses[which_series][k][dist][loss][10:]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    L = L.T\n",
    "    \n",
    "    D = np.diff(L, axis=1)\n",
    "    D = np.roll(D, 1, axis=0)[:-1]\n",
    "    \n",
    "    H = np.vstack([np.ones(T-1), D.T]).T\n",
    "    \n",
    "    mcs, S, cval, pval, removed = cmcs(L[:-1, :], H=H,covar_style=\"hac\", kernel=\"Parzen\",alpha=alpha)\n",
    "    \n",
    "    return mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6ffbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for a in [0.01,0.05,0.1]:\n",
    "    print(MCS_calculator(which_series='DAX',\n",
    "                  ARMA_losses=ARMA_GARCH,\n",
    "                  NN_Data= NN,\n",
    "                  loss='log',\n",
    "                  alpha=a))\n",
    "    print(MCS_calculator('DAX',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                 'quad',\n",
    "                  a))\n",
    "    print(MCS_calculator('DAX',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                         'sph',\n",
    "                  a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38589c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Only Laplace NN in 5% MCS, across all losses, yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98bd95ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for a in [0.01,0.05,0.1]:\n",
    "    print(MCS_calculator(which_series='Nikkei',\n",
    "                  ARMA_losses=ARMA_GARCH,\n",
    "                  NN_Data= NN,\n",
    "                  loss='log',\n",
    "                  alpha=a))\n",
    "    print(MCS_calculator('Nikkei',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                 'quad',\n",
    "                  a))\n",
    "    print(MCS_calculator('Nikkei',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                         'sph',\n",
    "                  a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b8c1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for a in [0.01,0.05,0.1]:\n",
    "    print(MCS_calculator(which_series='NASDAQ',\n",
    "                  ARMA_losses=ARMA_GARCH,\n",
    "                  NN_Data= NN,\n",
    "                  loss='log',\n",
    "                  alpha=a))\n",
    "    print(MCS_calculator('NASDAQ',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                 'quad',\n",
    "                  a))\n",
    "    print(MCS_calculator('NASDAQ',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                         'sph',\n",
    "                  a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ce990",
   "metadata": {},
   "source": [
    "# MCS Robustness Check\n",
    "Try out a different test function --> now use two lags of loss difference plus a lag of dependent var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12352261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCS_calculator_2(which_series,\n",
    "             ARMA_losses,\n",
    "              NN_Data,\n",
    "                   loss,\n",
    "                  alpha):\n",
    "    \n",
    "    T = len(NN_Data[which_series]['Student_T_losses_'+loss])\n",
    "    \n",
    "    L = np.vstack([\n",
    "                          NN_Data[which_series]['Student_T_losses_'+loss],\n",
    "                          NN_Data[which_series]['Laplace_losses_'+loss],\n",
    "                          NN_Data[which_series]['Normal_losses_'+loss]]\n",
    "                         )\n",
    "    \n",
    "    \n",
    "    for k in ARMA_losses[which_series].keys(): \n",
    "        for dist in ['T','Normal','Laplace']:\n",
    "\n",
    "            L = np.vstack([L, ARMA_losses[which_series][k][dist][loss][10:]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    L = L.T\n",
    "    \n",
    "    D = np.diff(L, axis=1)\n",
    "    D = np.roll(D, 1, axis=0)[:-1]\n",
    "    \n",
    "    D2 = np.roll(D,1,axis=0)\n",
    "    \n",
    "    lagged_dep = np.vstack([DATA[which_series]['Data']['log_ret'][-len(D2):]])\n",
    "    \n",
    "    lagged_dep = lagged_dep.T\n",
    "    \n",
    "    lagged_dep = np.roll(lagged_dep,1,axis=0)\n",
    "    \n",
    "    H = np.vstack([np.ones(T-1), D.T, D2.T,lagged_dep.T]).T\n",
    "    \n",
    "    mcs, S, cval, pval, removed = cmcs(L[:-1, :], H=H,covar_style=\"hac\", kernel=\"Parzen\",alpha=alpha)\n",
    "    \n",
    "    return mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "987b17cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for a in [0.01,0.05,0.1]:\n",
    "    print(MCS_calculator_2(which_series='DAX',\n",
    "                  ARMA_losses=ARMA_GARCH,\n",
    "                  NN_Data= NN,\n",
    "                  loss='log',\n",
    "                  alpha=a))\n",
    "    print(MCS_calculator_2('DAX',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                 'quad',\n",
    "                  a))\n",
    "    print(MCS_calculator_2('DAX',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                         'sph',\n",
    "                  a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c85dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for a in [0.01,0.05,0.1]:\n",
    "    print(MCS_calculator_2(which_series='Nikkei',\n",
    "                  ARMA_losses=ARMA_GARCH,\n",
    "                  NN_Data= NN,\n",
    "                  loss='log',\n",
    "                  alpha=a))\n",
    "    print(MCS_calculator_2('Nikkei',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                 'quad',\n",
    "                  a))\n",
    "    print(MCS_calculator_2('Nikkei',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                         'sph',\n",
    "                  a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78e4f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for a in [0.01,0.05,0.1]:\n",
    "    print(MCS_calculator_2(which_series='NASDAQ',\n",
    "                  ARMA_losses=ARMA_GARCH,\n",
    "                  NN_Data= NN,\n",
    "                  loss='log',\n",
    "                  alpha=a))\n",
    "    print(MCS_calculator_2('NASDAQ',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                 'quad',\n",
    "                  a))\n",
    "    print(MCS_calculator_2('NASDAQ',\n",
    "                         ARMA_GARCH,\n",
    "                  NN,\n",
    "                         'sph',\n",
    "                  a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
