Uncertainty is fundamental in economics. Imperfect or asymmetric information can lead to marked misallocations of resources, resulting in partial or total market failure, and their ubiquity is a hard truth of many economies. Forecasting stochastic events well is, therefore, of particular interest in econometrics and finance.

In the past, forecasting literature focused on point and interval forecasts as little practical use was seen in density forecasting and initial attempts to create tractable density forecasting techniques required restrictive assumptions, such as no parameter uncertainty or Gaussian innovations \citep{DieGunTay}. However, with the emergence of quantitative finance and risk management, appetite for accurate density forecasts has grown, leading to a burgeoning literature on the subject. Importantly, advancements in computer technology and simulation have resulted in usable density forecasts under minimal requirements. \citet{BOERO2004305} noted that at the time of their writing over half of all inflation targeting banks were using some form of density forecasting. The number of macroeconomic policymakers utilising density forecasting has likely grown significantly since then, and Value-at-Risk analysis has become foundational in modern risk management. 

Large datasets coupled with new computer technologies have meant that complex and highly non-linear models for density forecasting can now be accurately estimated. Intuitively, one would expect non-linear models to produce better forecasts than linear models, given thresholds, capacity constraints, and other asymmetries which engender non-linear decision boundaries \citep{DAHL2004201}. Despite this, it appears that non-linear models do not significantly outperform linear models, such as ARMA-GARCH, across the board and can potentially result in worse forecasts \citep{DEGOOIJER,CLEMENTS2004169}. The issue with highly non-linear models is related to the "curse of dimensionality" as slight perturbations to the underlying data generating process can impact the accuracy of forecasts significantly more in high dimensional models relative to smaller models \citep{diakonikolas2018robust}.

However, previous comparisons of linear and non-linear models are restricted as they focus on point forecasts \citep{DAHL2004201}, only consider regime-changing models \citep{CLEMENTS2004169}, or arbitrarily discretise the outcome space to convert a function approximation problem to a classification task \citep{IBM-DE-RNN}. To fill this literature gap, a novel neural network (NN) architecture is developed, based on a series of Long Short-Term Memory (LSTM) blocks \citep{LSTM} and allowing for a continuous outcome space. The paper compares ARMA-GARCH models up of order $(p,p)-(p,p)$, for $p=1,2,3,4,5$, with variants of the NN on the log returns of three globally large stock indices, viz. the NASDAQ composite (\textsuperscript{$\wedge$}$IXIC$), the DAX (\textsuperscript{$\wedge$}$GDAXI$), and Nikkei 225 (\textsuperscript{$\wedge$}$N225$). The NN model differs from other techniques used for density forecasting, which use non-parametric models, such as kernel density estimation, classification methods, or fully parametric models \citep{KernelDen, KernelDensity,IBM-DE-RNN}.  Instead, the NN model assumes an underlying parametrisation of the distribution a priori and parameters of the distribution are then estimated using a NN. Thus, the NN architecture is semi-parametric; although an underlying distribution for the data is assumed in each model, increasing the breadth of assumed distributions (e.g. assuming a parametric family of distributions rather than individual ones) and width of the NNs decreases the restrictions on the data generating process. Indeed, as the width of a NN tends to infinity, it can be considered as a Gaussian Process: a non-parametric model \citep{lee2018deep}.

At the inference stage, I test the null hypothesis ($H_0$) of equal conditional performance of the \textit{class} of NN models and the \textit{class} of ARMA-GARCH models against the two alternative hypotheses that either the NN class performs better ($H_1$) or the ARMA-GARCH class performs best ($H_2$). I use a multivariate version of the Giacomini-White test \citep{BorupMGW,GiacominiWhite} of equal predictive performance to construct a model confidence set (MCS) \citep{MCS} at the $95\%$ level. If both ARMA-GARCH and NNs are in the MCS ($H_0$) will be accepted, whilst if only NNs or ARMA-GARCH remain in the MCS, the null will be rejected in favour of $(H_1)$ or $(H_2)$, respectively.
