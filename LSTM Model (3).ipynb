{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d4ccb5",
   "metadata": {},
   "source": [
    "# Neural Network for Density Forecasting EC331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be40c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#######################################################################\n",
    "######################### Importing Packages ##########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# numpy and pandas\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from numpy import ix_\n",
    "import pandas as pd\n",
    "\n",
    "# miscellany to make code neater\n",
    "from typing import Callable\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# some basic statistical/numerical scipy parts used to calculate scores\n",
    "import scipy.stats\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "# Tensorflow and Keras parts which are used for data processing and model creation\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# tensorflow pdf calculator which is used to calculate losses efficiently\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# setting seed as stochastic intialisation\n",
    "tf.random.set_seed(11)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc574d2",
   "metadata": {},
   "source": [
    "# Data Importing and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2169fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#######################################################################\n",
    "###################### Data Importing/Cleaning ########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "main_path = \"C:/Warwick Final Year/RAE/\"\n",
    "raw_path = main_path + \"Data/\"\n",
    "processed_path = main_path + \"Processed Data/\"\n",
    "graphs_path = main_path + \"Graphs/\"\n",
    "checkpoints_path = main_path + \"Code Python/Model Checkpoints/\"\n",
    "\n",
    "# import data and calculate log returns from adjusted close\n",
    "df_Nikkei_RAW = pd.read_csv(raw_path + \"^N225.csv\")\n",
    "df_NASDAQ_RAW = pd.read_csv(raw_path + \"^IXIC.csv\")\n",
    "df_DAX_RAW = pd.read_csv(raw_path + \"^GDAXI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc572973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Processor(DATA: pd.DataFrame,\n",
    "                   batch_size: int,\n",
    "                   length_sample: int,\n",
    "                   test_train_split: str) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \n",
    "    Processes data to a usable form. Splits data into a training and test set based on date\n",
    "    given by 'test_train_split'. Makes data automatically batch and use a lag length of 'length_sample'.\n",
    "    \n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \n",
    "    @param DATA: pd.DataFrame, stock index data from Yahoo Finance with Adjusted Closing prices and Date as columns\n",
    "    @param batch_size: int, size of batch used whilst training models\n",
    "    @param length_sample: int, number of lagged trading days to use whilst forecasting each timestep\n",
    "    @param test_train_split: str, date at which to split data for training and validation - format of \"dd-mm-yyyy\"\n",
    "    \n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \n",
    "    @return: dict, dictionary with 3 keys. DATA key is the log return data for the given series after cleaning\n",
    "                                           Training key is the training data which is batched and has given sample size\n",
    "                                           Validation key is the validation data which is batch and has given sample size\n",
    "    \n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \"\"\"\n",
    "    \n",
    "    DATA.columns = [c.replace(' ', '_') for c in DATA.columns]\n",
    "    DATA = DATA[DATA['Adj_Close'].notnull()]\n",
    "    DATA['log_ret'] = np.log(DATA.Adj_Close) - np.log(DATA.Adj_Close.shift(1))\n",
    "\n",
    "    # spilt to training and test sets\n",
    "    DATA    = DATA[['Date', 'log_ret']][1:]\n",
    "    DATA['Date'] = DATA['Date'].apply(pd.Timestamp)\n",
    "    DATA.set_index('Date', inplace=True, drop=True)\n",
    "    \n",
    "    train = DATA.loc[:test_train_split]\n",
    "    test  = DATA.loc[test_train_split:]\n",
    "   \n",
    "    DATA_train = [[i] for i in train['log_ret']]\n",
    "    DATA_test  = [[i] for i in test['log_ret']]\n",
    "    \n",
    "    time_series_generator = TimeseriesGenerator(DATA_train, \n",
    "                                                DATA_train, \n",
    "                                                length = length_sample, \n",
    "                                                batch_size = batch_size)\n",
    "    time_series_val_generator = TimeseriesGenerator(DATA_test,\n",
    "                                                    DATA_test, \n",
    "                                                    length = length_sample, \n",
    "                                                    batch_size = batch_size)\n",
    "\n",
    "    \n",
    "    return {\"Data\": DATA,\n",
    "            \"Training\": time_series_generator,\n",
    "            \"Validation\": time_series_val_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lead in the Nikkei, NASDAQ, and DAX \n",
    "\n",
    "Batch_Size = 64\n",
    "Length = 10\n",
    "Test_Train_Split = '2015-01-01'\n",
    "\n",
    "Nikkei = Data_Processor(df_Nikkei_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "NASDAQ = Data_Processor(df_NASDAQ_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "DAX = Data_Processor(df_DAX_RAW,\n",
    "                       Batch_Size,\n",
    "                       Length,\n",
    "                       Test_Train_Split)\n",
    "\n",
    "DATA = {\n",
    "       \"Nikkei\": Nikkei,\n",
    "       \"NASDAQ\": NASDAQ,\n",
    "       \"DAX\": DAX\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3065e7",
   "metadata": {},
   "source": [
    "# Plotting Stock Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count,i in enumerate([Nikkei['Data'],\n",
    "         NASDAQ['Data'],\n",
    "         DAX['Data']]):\n",
    "    \n",
    "\n",
    "    if count == 0:\n",
    "        NAME = \"Nikkei\"\n",
    "    elif count == 1:\n",
    "        NAME = \"NASDAQ\"\n",
    "    else:\n",
    "        NAME = \"DAX\"\n",
    "    \n",
    "    ax = i.plot(y = 'log_ret', \n",
    "            kind = 'line',\n",
    "            rot = 45,\n",
    "            legend = False,\n",
    "            title = NAME\n",
    "           )\n",
    "    \n",
    "    \n",
    "    means = np.repeat(0, len(i['log_ret']))\n",
    "    std   = np.repeat(np.std(i['log_ret']), len(i['log_ret']))\n",
    "    x     = pd.date_range(str(i.index.values[0]), str(i.index.values[-1]), freq = \"D\")\n",
    "    ax.axvline(pd.to_datetime('2015-01-01'), color = 'r', linestyle = '--')\n",
    "    \n",
    "    s1 = ax.fill_between(i.index.values, \n",
    "                         np.add(means,std),\n",
    "                         np.subtract(means,std), \n",
    "                         color = 'green', \n",
    "                         zorder = 4, \n",
    "                         alpha = 0.4)\n",
    "    \n",
    "    s2 = ax.fill_between(i.index.values, \n",
    "                         np.add(means,np.multiply(2,std)),\n",
    "                         np.subtract(means,np.multiply(2,std)), \n",
    "                         color = 'grey',\n",
    "                         zorder = 3,\n",
    "                         alpha = 0.5)\n",
    "    \n",
    "    ax.text(pd.to_datetime('2016-06-01'), \n",
    "            -0.18,\n",
    "        \"Test/Train Split\",\n",
    "        horizontalalignment = 'center', \n",
    "            fontweight = 'bold', \n",
    "            color = 'red', \n",
    "            rotation = -90,\n",
    "           fontsize = 'x-small')\n",
    "    \n",
    "    years = mdates.YearLocator(10)   # every year\n",
    "    years_fmt = mdates.DateFormatter('%Y')\n",
    "    \n",
    "    \n",
    "    ax.xaxis.set_major_locator(years)\n",
    "    ax.xaxis.set_major_formatter(years_fmt)\n",
    "    ax.set(ylabel = \"Log Returns\")\n",
    "    ax.legend(handles = [s1,s2], labels = [\"1 std\",\"2 std\"], loc = 'upper left')\n",
    "    ax.set_ylim([-0.2, 0.2])\n",
    "    plt.tight_layout() \n",
    "    ax.figure.savefig(graphs_path + str(count) + 'logret.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb982d",
   "metadata": {},
   "source": [
    "# The Neural Network Architecture\n",
    "\n",
    "These neural network models work by assuming some parametrised distribution for the stock returns and forecasting the parameters of the distribution in the following period. \n",
    "\\\n",
    "As a loss function we use the log of the probability density function. Thus, our neural network is in effect being trained to converge to the maximum likelihood estimators. \n",
    "\\\n",
    "Basic building block is LSTM layers interspersed with dropout layers for regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74399194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, distribution):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.distribution = distribution\n",
    "        \n",
    "        # conditional mean channel\n",
    "        self.LSTM1 = tf.keras.layers.LSTM(16, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        self.LSTM2 = tf.keras.layers.LSTM(32, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        self.LSTM3 = tf.keras.layers.LSTM(16, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences=False)\n",
    "        \n",
    "        \n",
    "        # scale/std channel \n",
    "        self.LSTM4 = tf.keras.layers.LSTM(16, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences=True)\n",
    "        \n",
    "        self.LSTM5 = tf.keras.layers.LSTM(32, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences = True)\n",
    "        \n",
    "        \n",
    "        self.LSTM6 = tf.keras.layers.LSTM(16, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences = False)\n",
    "        \n",
    "        # DoF Channel\n",
    "        \n",
    "        self.LSTM7 = tf.keras.layers.LSTM(16, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences = True)\n",
    "        \n",
    "        self.LSTM8 = tf.keras.layers.LSTM(32, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences = True)\n",
    "        \n",
    "        \n",
    "        self.LSTM9 = tf.keras.layers.LSTM(16, \n",
    "                                          activation = tf.nn.relu,\n",
    "                                          return_sequences = False)\n",
    "\n",
    "\n",
    "        # dropout layers to regularise\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.4)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        self.dropout4 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout5 = tf.keras.layers.Dropout(0.4)\n",
    "        self.dropout6 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        self.dropout7 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout8 = tf.keras.layers.Dropout(0.4)\n",
    "        self.dropout9 = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "\n",
    "\n",
    "        # a dense layer for conditional mean\n",
    "        self.dense1 = tf.keras.layers.Dense(1, \n",
    "                                            activation='tanh')\n",
    "        \n",
    "        # another for the scale parameter\n",
    "        self.dense2 = tf.keras.layers.Dense(1, \n",
    "                                            activation='sigmoid')\n",
    "        \n",
    "        # for t-distribution the scale parameter doesn't correspond to std \n",
    "        # DoF >0 and scale >0 so use relu for these\n",
    "        self.dense3 = tf.keras.layers.Dense(1,\n",
    "                                           activation='relu')\n",
    "        \n",
    "        self.dense4 = tf.keras.layers.Dense(1,\n",
    "                                           activation='relu')\n",
    "        \n",
    "    \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # LSTM --> Dropout --> dense with 2 outputs (conditional mean & std)\n",
    "        \n",
    "        if self.distribution in ['Normal',\"Laplace\"] :\n",
    "        \n",
    "            # mean channel\n",
    "            x1 = self.LSTM1(inputs)\n",
    "            x1 = self.dropout1(x1)    \n",
    "            x1 = self.LSTM2(x1)\n",
    "            x1 = self.dropout2(x1)\n",
    "            x1 = self.LSTM3(x1)\n",
    "            x1 = self.dropout3(x1)\n",
    "\n",
    "            # std channel\n",
    "            x2 = self.LSTM4(inputs)\n",
    "            x2 = self.dropout4(x2)\n",
    "            x2 = self.LSTM5(x2)\n",
    "            x2 = self.dropout5(x2)\n",
    "            x2 = self.LSTM6(x2)\n",
    "            x2 = self.dropout6(x2)\n",
    "\n",
    "            out1 = self.dense1(x1)\n",
    "            out2 = self.dense2(x2)\n",
    "\n",
    "            return concatenate([out1,out2])\n",
    "        \n",
    "        elif self.distribution == 't':\n",
    "            \n",
    "             # mean channel\n",
    "            x1 = self.LSTM1(inputs)\n",
    "            x1 = self.dropout1(x1)    \n",
    "            x1 = self.LSTM2(x1)\n",
    "            x1 = self.dropout2(x1)\n",
    "            x1 = self.LSTM3(x1)\n",
    "            x1 = self.dropout3(x1)\n",
    "\n",
    "            # scale channel\n",
    "            x2 = self.LSTM4(inputs)\n",
    "            x2 = self.dropout4(x2)\n",
    "            x2 = self.LSTM5(x2)\n",
    "            x2 = self.dropout5(x2)\n",
    "            x2 = self.LSTM6(x2)\n",
    "            x2 = self.dropout6(x2)\n",
    "            \n",
    "            # DoF channel for t dist\n",
    "            \n",
    "            x3 = self.LSTM7(inputs)\n",
    "            x3 = self.dropout7(x3)\n",
    "            x3 = self.LSTM8(x3)\n",
    "            x3 = self.dropout8(x3)\n",
    "            x3 = self.LSTM9(x3)\n",
    "            x3 = self.dropout9(x3)\n",
    "\n",
    "            out1 = self.dense1(x1)\n",
    "            out2 = self.dense3(x2)\n",
    "            out3 = self.dense4(x3)\n",
    "\n",
    "            return concatenate([out1,out2,out3])\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(distribution: str) -> Callable[[list,list], float]:\n",
    "    \n",
    "    \"\"\"\n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \n",
    "    Generates the loss function for the specified distribution. Adds small numbers (1e-10) to some parameters\n",
    "    so that learning doesn't produce NaN accidentally and stop learning. There is a way to integrate this\n",
    "    with the above class but it keeps throwing errors when I try so forgive the sloppy code!\n",
    "    \n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \n",
    "    @param distribution: str, the chosen distribution to generate the loss function for\n",
    "    \n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \n",
    "    @return loss_comp: loss function taking in the y_true target variable and the predicted y_pred variables\n",
    "                       and outputs the average loss over the batch\n",
    "                       \n",
    "    #################################################################################################################\n",
    "    #################################################################################################################\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if distribution == \"Normal\":\n",
    "        def loss_comp(y_true, y_pred):\n",
    "            (mean_pred, scale_pred) = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "            dist = tfd.Normal(loc = mean_pred, scale=tf.math.add(scale_pred,1e-10))   \n",
    "            log_dens = tf.math.log(tf.math.add(dist.prob(y_true),1e-10))\n",
    "            loss = -tf.math.reduce_mean(log_dens)\n",
    "            return loss\n",
    "            \n",
    "    elif distribution == \"t\":\n",
    "        def loss_comp(y_true, y_pred):\n",
    "            (mean_pred, scale_pred, nu_pred) = tf.split(y_pred, num_or_size_splits=3, axis=1)\n",
    "            dist = tfd.StudentT(df = tf.math.add(nu_pred,1e-10), loc = mean_pred, scale=tf.math.add(scale_pred,1e-10))   \n",
    "            log_dens = tf.math.log(dist.prob(y_true))\n",
    "            loss = -tf.math.reduce_mean(log_dens)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "    elif distribution == \"Laplace\":\n",
    "        def loss_comp(y_true, y_pred):\n",
    "            (mean_pred, scale_pred) = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "            dist = tfd.Laplace(loc = mean_pred, \n",
    "                               scale=tf.math.add(scale_pred,1e-10))   \n",
    "            log_dens = tf.math.log(tf.math.add(dist.prob(y_true),1e-10))\n",
    "            loss = -tf.math.reduce_mean(log_dens)\n",
    "            return loss\n",
    "        \n",
    "    return loss_comp\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks\n",
    "callbacks = {}\n",
    "for i in [\"Normal\",\"t\",\"Laplace\"]:\n",
    "\n",
    "    callbacks[i] = [ ModelCheckpoint(filepath=checkpoints_path + i,\n",
    "                                         monitor='val_loss',\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=True,\n",
    "                                         save_best_only=True),\n",
    "                    ReduceLROnPlateau(monitor='val_loss',\n",
    "                                              factor=0.1,\n",
    "                                              min_lr=1e-10,\n",
    "                                              patience=10,\n",
    "                                              verbose=1),\n",
    "                   EarlyStopping(monitor='loss', patience=50)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea60a7",
   "metadata": {},
   "source": [
    "# Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aefd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick which series to fit:\n",
    "\n",
    "which_series = \"Nikkei\"\n",
    "\n",
    "if which_series not in [\"Nikkei\",\"NASDAQ\",\"DAX\"]:\n",
    "    raise Exception(\"That series is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4692f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_generator = DATA[which_series][\"Training\"]\n",
    "time_series_val_generator = DATA[which_series]['Validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00816747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Hyperparameters for training\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "learning_rates = {\n",
    "                  \"t\"       : 0.0001,\n",
    "                  \"Normal\"  : 0.01,\n",
    "                  \"Laplace\" : 0.001\n",
    "}\n",
    "\n",
    "loss_functions = {\n",
    "                  \"t\"       : loss_func(\"t\"),\n",
    "                  \"Normal\"  : loss_func(\"Normal\"),\n",
    "                  \"Laplace\" : loss_func(\"Laplace\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we fit the model using a t-distribution\n",
    "model_t = Model(\"t\")\n",
    "\n",
    "model_t.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rates[\"t\"]),\n",
    "                loss=loss_functions[\"t\"])\n",
    "\n",
    "\n",
    "t_History = model_t.fit(time_series_generator,        \n",
    "                        epochs = num_epochs, \n",
    "                        shuffle = False,\n",
    "                        validation_data=time_series_val_generator,\n",
    "                        callbacks=callbacks[\"t\"])\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d74440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we fit the model with a normal distribution\n",
    "\n",
    "model_Normal = Model(\"Normal\")\n",
    "\n",
    "model_Normal.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rates[\"Normal\"]),\n",
    "                     loss=loss_functions[\"Normal\"])\n",
    "\n",
    "\n",
    "Normal_History = model_Normal.fit(time_series_generator,        \n",
    "                                  epochs = num_epochs, \n",
    "                                  shuffle = False,\n",
    "                                  validation_data=time_series_val_generator,\n",
    "                                  callbacks=callbacks[\"Normal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e30349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally fit the model with a Laplace distribution\n",
    "\n",
    "model_Laplace = Model(\"Laplace\")\n",
    "\n",
    "model_Laplace.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rates[\"Laplace\"]),\n",
    "                      loss=loss_functions[\"Laplace\"])\n",
    "\n",
    "Laplace_History = model_Laplace.fit(time_series_generator,        \n",
    "                                    epochs = num_epochs, \n",
    "                                    shuffle = False,\n",
    "                                    validation_data=time_series_val_generator,\n",
    "                                    callbacks=callbacks[\"Laplace\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439b740",
   "metadata": {},
   "source": [
    "# Plotting Forecasts\n",
    "Plot 5% and 10% VaR bands for each forecasted distribution and compared to observed stock returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd978f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_Pred_t     = model_t.predict(DATA[which_series]['Validation'])\n",
    "Val_Pred_Normal = model_Normal.predict(DATA[which_series]['Validation'])\n",
    "Val_Pred_Laplace = model_Laplace.predict(DATA[which_series]['Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set is from 2015-01-01, also need length of window forecasting over to predict first data point\n",
    "validation_data = DATA[which_series][\"Data\"].loc['2015-01-01':]['log_ret'][Length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9b99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first plot Normal\n",
    "means_N = [i[0] for i in Val_Pred_Normal]\n",
    "std_N   = [i[1] for i in Val_Pred_Normal]\n",
    "x = pd.date_range(\"2015-01-01\", periods=len(means_N),freq=\"D\")\n",
    "\n",
    "\n",
    "plt.plot(x,validation_data, color='#1f77b4',zorder=1)\n",
    "\n",
    "ppf = scipy.stats.norm.ppf\n",
    "SCORE = -round(Normal_History.history[\"val_loss\"][-1],4)\n",
    "NAME = \"Normal\"\n",
    "s1 = plt.fill_between(x,np.add(means_N,ppf(0.95,loc=means_N,scale=std_N)),np.add(means_N,ppf(0.05,loc=means_N,scale=std_N)), \n",
    "                      color='green',zorder=4,alpha=0.4)\n",
    "s2 = plt.fill_between(x,np.add(means_N,ppf(0.975,loc=means_N,scale=std_N)),\n",
    "                 np.add(means_N,ppf(0.025,loc=means_N,scale=std_N)), \n",
    "                 color='grey',\n",
    "                     zorder=3,\n",
    "                     alpha=0.5)\n",
    "plt.plot(x,means_N, color='black',zorder=2)\n",
    "\n",
    "plt.title(NAME)\n",
    "\n",
    "plt.text(pd.to_datetime('2019-06-01'), \n",
    "        -0.08,\n",
    "    \"SCORE=\"+str(SCORE),\n",
    "    horizontalalignment='center', \n",
    "        fontweight='bold', \n",
    "        color='red',\n",
    "       fontsize='medium')\n",
    "\n",
    "years = mdates.YearLocator(10)  \n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "\n",
    "plt.ylabel(\"Log Returns\")\n",
    "plt.legend(handles=[s1,s2], labels=[\"10%\",\"5%\"],loc='upper right')\n",
    "plt.ylim([-0.15, 0.15])\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig(graphs_path + which_series + '_Validation_' + NAME + '.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf657b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now t\n",
    "means_T = [i[0] for i in Val_Pred_t]\n",
    "scale_T   = [i[1] for i in Val_Pred_t]\n",
    "dfs_T   = [i[2] for i in Val_Pred_t]\n",
    "x = pd.date_range(\"2015-01-01\", periods=len(means_T),freq=\"D\")\n",
    "\n",
    "\n",
    "plt.plot(x,validation_data, color='#1f77b4',zorder=1)\n",
    "\n",
    "ppf = scipy.stats.t.ppf\n",
    "         \n",
    "SCORE = -round(t_History.history[\"val_loss\"][-1],4)\n",
    "NAME = \"Student_T\"\n",
    "         \n",
    "s1 = plt.fill_between(x,np.add(means_T,ppf(0.95,df=dfs_T, loc=means_T,scale=scale_T)),\n",
    "                      np.add(means_T,ppf(0.05,df=dfs_T,loc=means_T,scale=scale_T)), \n",
    "                      color='green',zorder=4,alpha=0.4)\n",
    "s2 = plt.fill_between(x,np.add(means_T,ppf(0.975,df=dfs_T,loc=means_T,scale=scale_T)),\n",
    "                 np.add(means_T,ppf(0.025,df=dfs_T,loc=means_T,scale=scale_T)), \n",
    "                 color='grey',\n",
    "                     zorder=3,\n",
    "                     alpha=0.5)\n",
    "plt.plot(x,means_T, color='black',zorder=2)\n",
    "\n",
    "plt.title(NAME)\n",
    "\n",
    "plt.text(pd.to_datetime('2019-06-01'), \n",
    "        -0.08,\n",
    "    \"SCORE=\"+str(SCORE),\n",
    "    horizontalalignment='center', \n",
    "        fontweight='bold', \n",
    "        color='red',\n",
    "       fontsize='medium')\n",
    "\n",
    "years = mdates.YearLocator(10)  \n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "plt.ylabel(\"Log Returns\")\n",
    "plt.legend(handles=[s1,s2], labels=[\"10%\",\"5%\"],loc='upper right')\n",
    "plt.ylim([-0.15, 0.15])\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig(graphs_path + which_series + '_Validation_' + 'Student_T' + '.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0981dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly the Laplacian\n",
    "means_L = [i[0] for i in Val_Pred_Laplace]\n",
    "scale_L   = [i[1] for i in Val_Pred_Laplace]\n",
    "x = pd.date_range(\"2015-01-01\", periods=len(means_L),freq=\"D\")\n",
    "\n",
    "\n",
    "plt.plot(x,validation_data, color='#1f77b4',zorder=1)\n",
    "\n",
    "ppf = scipy.stats.laplace.ppf\n",
    "         \n",
    "SCORE = -round(Laplace_History.history[\"val_loss\"][-1],4)\n",
    "NAME = \"Laplace\"\n",
    "         \n",
    "s1 = plt.fill_between(x,np.add(means_L,ppf(0.95,loc=means_L,scale=scale_L)),\n",
    "                      np.add(means_L,ppf(0.05,loc=means_L,scale=scale_L)), \n",
    "                      color='green',zorder=4,alpha=0.4)\n",
    "s2 = plt.fill_between(x,np.add(means_L,ppf(0.975,loc=means_L,scale=scale_L)),\n",
    "                 np.add(means_L,ppf(0.025,loc=means_L,scale=scale_L)), \n",
    "                 color='grey',\n",
    "                     zorder=3,\n",
    "                     alpha=0.5)\n",
    "plt.plot(x,means_L, color='black',zorder=2)\n",
    "\n",
    "plt.title(NAME)\n",
    "\n",
    "plt.text(pd.to_datetime('2019-06-01'), \n",
    "        -0.08,\n",
    "    \"SCORE=\"+str(SCORE),\n",
    "    horizontalalignment='center', \n",
    "        fontweight='bold', \n",
    "        color='red',\n",
    "       fontsize='medium')\n",
    "\n",
    "years = mdates.YearLocator(10)  \n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "plt.ylabel(\"Log Returns\")\n",
    "plt.legend(handles=[s1,s2], labels=[\"10%\",\"5%\"],loc='upper right')\n",
    "plt.ylim([-0.15, 0.15])\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig(graphs_path + which_series + '_Validation_' + NAME + '.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f224bac",
   "metadata": {},
   "source": [
    "# Exporting losses and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers \n",
    "Normal_pdfs = scipy.stats.norm.pdf(\n",
    "                                   x=DAX['Data'].loc['2015-01-01':]['log_ret'][10:],\n",
    "                                   loc=means_N,\n",
    "                                   scale=std_N\n",
    "                                )\n",
    "\n",
    "T_pdfs = scipy.stats.t.pdf(\n",
    "                                   x=DAX['Data'].loc['2015-01-01':]['log_ret'][10:],\n",
    "                                   df=dfs_T,\n",
    "                                   loc=means_T,\n",
    "                                   scale=scale_T\n",
    "                                )\n",
    "\n",
    "Laplace_pdfs = scipy.stats.laplace.pdf(\n",
    "                                   x=DAX['Data'].loc['2015-01-01':]['log_ret'][10:],\n",
    "                                   loc=means_L,\n",
    "                                   scale=scale_L\n",
    "                                )\n",
    "\n",
    "Normal_L2 =[0 for i in range(len(means_N))]\n",
    "T_L2 = [0 for i in range(len(means_N))]\n",
    "Laplace_L2 =[0 for i in range(len(means_N))]\n",
    "\n",
    "for i in range(len(means_N)):\n",
    "    \n",
    "    Normal_L2[i] = np.power(integrate.quad(lambda x: np.power(scipy.stats.norm.pdf(x,\n",
    "                                                                              loc=means_N[i],\n",
    "                                                                              scale=std_N[i]),2),-np.inf,np.inf)[0] , 0.5)\n",
    "    \n",
    "    T_L2[i] = np.power(integrate.quad(lambda x: np.power(scipy.stats.t.pdf(x,\n",
    "                                                                      df=dfs_T[i],\n",
    "                                                                      loc=means_T[i],\n",
    "                                                                      scale=scale_T[i]),2),-np.inf,np.inf)[0] , 0.5)\n",
    "    \n",
    "    Laplace_L2[i] = np.power(integrate.quad(lambda x: np.power(scipy.stats.laplace.pdf(x,\n",
    "                                                                               loc=means_L[i],\n",
    "                                                                               scale=scale_L[i]),2),-np.inf,np.inf)[0] , 0.5)\n",
    "\n",
    "\n",
    "# get the series of validation log losses for the three models\n",
    "\n",
    "T_losses_log       =  -np.log(T_pdfs\n",
    "                         )\n",
    "\n",
    "Normal_losses_log  = -np.log(Normal_pdfs\n",
    "                        )\n",
    "\n",
    "Laplace_losses_log = -np.log(Laplace_pdfs\n",
    "                        )\n",
    "\n",
    "# also the quad losses\n",
    "\n",
    "Normal_losses_Quad = -(np.multiply(Normal_pdfs,2)-\n",
    "                                  np.power(Normal_L2,2))\n",
    "            \n",
    "Laplace_losses_Quad = -(np.multiply(Laplace_pdfs,2)-\n",
    "                      np.power(Laplace_L2,2))\n",
    "\n",
    "\n",
    "T_losses_Quad = -(np.multiply(T_pdfs,2)-\n",
    "                      np.power(T_L2,2))\n",
    "#finally the spherical scores\n",
    "\n",
    "Normal_losses_Sph = -np.divide(Normal_pdfs,Normal_L2)\n",
    "\n",
    "Laplace_losses_Sph = -np.divide(Laplace_pdfs,Laplace_L2)\n",
    "\n",
    "T_losses_Sph = -np.divide(T_pdfs,T_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'Student_T_losses_log' : T_losses_log,\n",
    "    'Normal_losses_log'    : Normal_losses_log,\n",
    "    'Laplace_losses_log'   : Laplace_losses_log,\n",
    "    'Student_T_losses_quad' : T_losses_Quad,\n",
    "    'Normal_losses_quad'    : Normal_losses_Quad,\n",
    "    'Laplace_losses_quad'   : Laplace_losses_Quad,\n",
    "    'Student_T_losses_sph' : T_losses_Sph,\n",
    "    'Normal_losses_sph'    : Normal_losses_Sph,\n",
    "    'Laplace_losses_sph'   : Laplace_losses_Sph,\n",
    "    'Normal_loc'       : means_N,\n",
    "    'Normal_scale'     : std_N,\n",
    "    'Student_T_loc'    : means_T,\n",
    "    'Student_T_scale'  : scale_T,\n",
    "    'Student_T_shape'  : dfs_T,\n",
    "    'Laplace_loc'      : means_L,\n",
    "    'Laplace_scale'    : scale_L\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0179f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(processed_path + which_series + '_predictions_losses.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
